



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.4.3">
    
    
      
        <title>An Introduction to Probability and Computational Bayesian Statistics - Essays on Data Science</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/application.30686662.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#03a9f4">
      
    
    
      <script src="../../assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="https://unpkg.com/mermaid@7.1.2/dist/mermaid.css">
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "UA-12498603-3", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="light-blue" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#an-introduction-to-probability-and-computational-bayesian-statistics" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="../.." title="Essays on Data Science" class="md-header-nav__button md-logo">
          
            <i class="md-icon">library_books</i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Essays on Data Science
            </span>
            <span class="md-header-nav__topic">
              
                An Introduction to Probability and Computational Bayesian Statistics
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/ericmjl/essays-on-data-science/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="../.." title="Essays on Data Science" class="md-nav__button md-logo">
      
        <i class="md-icon">library_books</i>
      
    </a>
    Essays on Data Science
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/ericmjl/essays-on-data-science/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    ericmjl/essays-on-data-science
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="Essays on Data Science" class="md-nav__link">
      Essays on Data Science
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Computing
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Computing
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../computing/recursion/" title="Recursion" class="md-nav__link">
      Recursion
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      Machine learning
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        Machine learning
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        An Introduction to Probability and Computational Bayesian Statistics
      </label>
    
    <a href="./" title="An Introduction to Probability and Computational Bayesian Statistics" class="md-nav__link md-nav__link--active">
      An Introduction to Probability and Computational Bayesian Statistics
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    Probability Distributions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#base-object-implementation" class="md-nav__link">
    Base Object Implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability-density-function" class="md-nav__link">
    Probability Density Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-probability" class="md-nav__link">
    Log Probability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    Random Variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition" class="md-nav__link">
    Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#realizations-of-a-random-variable" class="md-nav__link">
    Realizations of a Random Variable
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-generating-process" class="md-nav__link">
    Data Generating Process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    Bayes' Rule
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#translating-bayes-math-to-python" class="md-nav__link">
    Translating Bayes' Math to Python
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#defining-posterior-log-likelihood" class="md-nav__link">
    Defining Posterior Log-Likelihood
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-posterior-with-sampling" class="md-nav__link">
    Computing the Posterior with Sampling
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-sampling" class="md-nav__link">
    Metropolis-Hastings Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformations-as-a-hack" class="md-nav__link">
    Transformations as a Hack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#samples-from-posterior" class="md-nav__link">
    Samples from Posterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#topics-we-skipped-over" class="md-nav__link">
    Topics We Skipped Over
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#an-anchoring-thought-framework-for-learning-computational-bayes" class="md-nav__link">
    An Anchoring Thought Framework for Learning Computational Bayes
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../message-passing/" title="Computational Representations of Message Passing" class="md-nav__link">
      Computational Representations of Message Passing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../reimplementing-models/" title="Reimplementing and Testing Deep Learning Models" class="md-nav__link">
      Reimplementing and Testing Deep Learning Models
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Software skills
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Software skills
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-skills/" title="Software Skills" class="md-nav__link">
      Software Skills
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-skills/code-formatting/" title="Formatting your code" class="md-nav__link">
      Formatting your code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-skills/documentation/" title="Documenting your code" class="md-nav__link">
      Documenting your code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-skills/refactoring/" title="Refactoring your code" class="md-nav__link">
      Refactoring your code
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../software-skills/testing/" title="Testing your code" class="md-nav__link">
      Testing your code
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      Terminal
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        Terminal
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../terminal/cli-tools/" title="Tools and Upgrades for your CLI" class="md-nav__link">
      Tools and Upgrades for your CLI
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../terminal/pre-commits/" title="Using `pre-commit` git hooks to automate code checks" class="md-nav__link">
      Using `pre-commit` git hooks to automate code checks
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6">
    
    <label class="md-nav__link" for="nav-6">
      Workflow
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-6">
        Workflow
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../workflow/code-review/" title="Practicing Code Review" class="md-nav__link">
      Practicing Code Review
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../workflow/effective-commit-messages/" title="Effective Git Commits in Data Science" class="md-nav__link">
      Effective Git Commits in Data Science
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../workflow/gitflow/" title="Principled Git-based Workflow in Collaborative Data Science Projects" class="md-nav__link">
      Principled Git-based Workflow in Collaborative Data Science Projects
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability-distributions" class="md-nav__link">
    Probability Distributions
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#base-object-implementation" class="md-nav__link">
    Base Object Implementation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability-density-function" class="md-nav__link">
    Probability Density Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#log-probability" class="md-nav__link">
    Log Probability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#random-variables" class="md-nav__link">
    Random Variables
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#definition" class="md-nav__link">
    Definition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#realizations-of-a-random-variable" class="md-nav__link">
    Realizations of a Random Variable
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-generating-process" class="md-nav__link">
    Data Generating Process
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bayes-rule" class="md-nav__link">
    Bayes' Rule
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#translating-bayes-math-to-python" class="md-nav__link">
    Translating Bayes' Math to Python
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#defining-posterior-log-likelihood" class="md-nav__link">
    Defining Posterior Log-Likelihood
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-the-posterior-with-sampling" class="md-nav__link">
    Computing the Posterior with Sampling
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#metropolis-hastings-sampling" class="md-nav__link">
    Metropolis-Hastings Sampling
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformations-as-a-hack" class="md-nav__link">
    Transformations as a Hack
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#samples-from-posterior" class="md-nav__link">
    Samples from Posterior
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#topics-we-skipped-over" class="md-nav__link">
    Topics We Skipped Over
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#an-anchoring-thought-framework-for-learning-computational-bayes" class="md-nav__link">
    An Anchoring Thought Framework for Learning Computational Bayes
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/ericmjl/essays-on-data-science/edit/master/docs/machine-learning/computational-bayesian-stats.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="an-introduction-to-probability-and-computational-bayesian-statistics">An Introduction to Probability and Computational Bayesian Statistics</h1>
<p>In Bayesian statistics,
we often say that we are "sampling" from a posterior distribution
to estimate what parameters could be,
given a model structure and data.
What exactly is happening here?</p>
<p>Examples that I have seen on "how sampling happens"
tends to focus on an overly-simple example
of sampling from a single distribution with known parameters.
I was wondering if I could challenge myself
to come up with a "simplest complex example"
that would illuminate ideas that were obscure to me before.
In this essay, I would like to share that knowledge with you,
and hopefully build up your intuition behind
what is happening in computational Bayesian inference.</p>
<h2 id="probability-distributions">Probability Distributions</h2>
<p>We do need to have a working understanding
of what a probability distribution is before we can go on.
Without going down deep technical and philosophical rabbit holes
(I hear they are deep),
I'll start by proposing
that "a probability distribution is a Python object
that has a math function
that allocates credibility points onto the number line".</p>
<p>Because we'll be using the normal distribution extensively in this essay,
we'll start off by examining that definition
in the context of the standard normal distribution.</p>
<h3 id="base-object-implementation">Base Object Implementation</h3>
<p>Since the normal distribution is an object,
I'm implying here that it can hold state.
What might that state be?
Well, we know from math that probability distributions have parameters,
and that the normal distribution
has the "mean" and "variance" parameters defined.
In Python code, we might write it as:</p>
<div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
</pre></div>

<h3 id="probability-density-function">Probability Density Function</h3>
<p>Now, I also stated that the normal distribution has a math function
that we can use to allocate credibility points to the number line.
This function also has a name,
called a "probability distribution function", or the "PDF".
Using this, we may then extend extend this object
with a method called <code>.pdf(x)</code>,
that returns a number
giving the number of credibility points
assigned to the value of <code>x</code> passed in.</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
            <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
                <span class="o">-</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="p">))</span>
</pre></div>

<p>If we pass in a number <code>x</code> from the number line,
we will get back another number that tells us
the number of credibility points given to that value <code>x</code>,
under the state of the normal distribution instantiated.
We'll call this <span><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span>.</p>
<p>To simplify the implementation used here,
we are going to borrow some machinery already available to us
in the Python scientific computing ecosystem,
particularly from the SciPy stats module,
which gives us reference implementations of probability distributions.</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Now, our PDF class method is simplified to be just a wrapper.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

<h3 id="log-probability">Log Probability</h3>
<p>A common task in Bayesian inference is computing the likelihood of data.
Let's assume that the data <span><span class="MathJax_Preview">{X_1, X_2, ... X_i}</span><script type="math/tex">{X_1, X_2, ... X_i}</script></span> generated
are independent and identically distributed,
(the famous <em>i.i.d.</em> term comes from this).
This means, then, that the joint probability of the data that was generated
is equivalent to the product of the individual probabilities of each datum:</p>
<div>
<div class="MathJax_Preview">P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)</div>
<script type="math/tex; mode=display">P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)</script>
</div>
<p>(We have to know the rules of probability to know this result;
it is a topic for a different essay.)</p>
<p>If you remember the notation above,
each <span><span class="MathJax_Preview">P(X_i)</span><script type="math/tex">P(X_i)</script></span> is an evaluation of <span><span class="MathJax_Preview">X_i</span><script type="math/tex">X_i</script></span>
on the distribution's probability density function.
It being a probability value means it is bound between 0 and 1.
However, multiplying many probabilities together
usually will result in issues with underflow computationally,
so in evaluating likelihoods,
we usually stick with log-likelihoods instead.
By the usual rules of math, then:</p>
<div>
<div class="MathJax_Preview">\log P(X_1, X_2, ..., X_i) = \sum_{j=1}^{i}\log P(X_i)</div>
<script type="math/tex; mode=display">\log P(X_1, X_2, ..., X_i) = \sum_{j=1}^{i}\log P(X_i)</script>
</div>
<p>To our normal distribution class,
we can now add in another class method
that computes the sum of log likelihoods
evaluated at a bunch of i.i.d. data points.</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Now, our PDF class method is simplified to be just a wrapper.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

<h2 id="random-variables">Random Variables</h2>
<h3 id="definition">Definition</h3>
<p>Informally, a "random variable" is nothing more than
a variable whose quantity is non-deterministic (hence random)
but whose probability of taking on a certain value
can be described by a probability distribution.</p>
<p>According to the Wikipedia definition of a <a href="https://en.wikipedia.org/wiki/Random_variable">random variable</a>:</p>
<blockquote>
<p>A random variable has a probability distribution, which specifies the probability of its values.</p>
</blockquote>
<p>As such, it may be tempting to conceive of a random variable
as an object that has a probability distribution attribute attached to it.</p>
<h3 id="realizations-of-a-random-variable">Realizations of a Random Variable</h3>
<p>On the other hand, it can also be convenient to invert that relationship,
and claim that a probability distribution
can generate realizations of a random variable.
The latter is exactly how SciPy distributions are implemented:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Normal distribution can generate realizations of an RV</span>
<span class="c1"># The following returns a NumPy array of 10 draws</span>
<span class="c1"># from a standard normal distribution.</span>
<span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

<details class="note"><summary>Realizations of a Random Variable</summary><p>A "realization" of a random variable is nothing more than
generating a random number
whose probability of being generated
is defined by the random variable's probability density function.</p>
</details>
<p>Because the generation of realizations of a random variable
is equivalent to sampling from a probability distribution,
we can extend our probability distribution definition
to include a <code>.sample(n)</code> method:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">class</span> <span class="nc">Normal</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

        <span class="c1"># We instantiate the distribution object here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>

    <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></div>

<p>Now, if we draw 10 realizations of a normally distributed random variable,
and the drawing of each realization has no dependence of any kind
on the previous draw,
then we can claim that each draw is <strong>independent</strong>
and <strong>identically distributed</strong>.
This is where the fabled "<em>iid</em>" term in undergraduate statistics classes
comes from.</p>
<h2 id="data-generating-process">Data Generating Process</h2>
<p>Now that we have covered what probability distributions are,
we can now move on to other concepts
that are important in Bayesian statistical modelling.</p>
<p>Realizations of a random variable,
or draws from its probability distribution,
are how a Bayesian assumes data are generated.
Describing how data are generated using probability distributions,
or in other words, writing down the "data generating process",
is a core activity in Bayesian statistical modelling.</p>
<p>Viewed this way, data values generated by a random process
depend on the underlying random variable's probability distribution.
In other words, the random variable realizations are known,
given the probability distribution used to model it.
Keep this idea in mind:
it is going to be important shortly.</p>
<h2 id="bayes-rule">Bayes' Rule</h2>
<p>Now that we've covered probability distributions,
we can move on to Bayes' rule.
You probably have seen the following equation:</p>
<div>
<div class="MathJax_Preview">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</div>
<script type="math/tex; mode=display">P(B|A) = \frac{P(A|B)P(B)}{P(A)}</script>
</div>
<p>Bayes' rule states nothing more than the fact that
the conditional probability of B given A is equal to
the conditional probability of A given B
times the probability of B
divided by the probability of A.</p>
<p>When doing Bayesian statistical inference,
we commonly take a related but distinct interpretation:</p>
<div>
<div class="MathJax_Preview">P(H|D) = \frac{P(D|H)P(H)}{P(D)}</div>
<script type="math/tex; mode=display">P(H|D) = \frac{P(D|H)P(H)}{P(D)}</script>
</div>
<p>It may look weird,
but didn't we say before that data are realizations from a random variable?
Why are we now treating data as a random variable?
Here, we are doing not-so-intuitive but technically correct step
of treating the data <span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> as being part of this probabilistic model
(hence it "looks" like a random variable),
alongside our model parameters <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>.
There's a lot of measure theory that goes into this interpretation,
which at this point I have not yet mastered,
and so will wave my hands in great arcs
and propose that this interpretation be accepted for now and move on.</p>
<details class="note"><summary>Data are random variables?</summary><p>Notes from a chat with Colin gave me a lot to chew on, as usual:</p>
<blockquote>
<p>The answer is in how you define "event" as
"an element of a sigma algebra".
intuitively, an "event" is just an abstraction,
so one event might be "the coin is heads",
or in another context the event might be
"the parameters are [0.2, 0.1, 0.2]".
And so analogously, "the data were configured as [0, 5, 2, 3]".
Notice also that the events are different
if the data being ordered vs unordered are different!</p>
</blockquote>
<p>This was a logical leap that I had been asked about before,
but did not previously have the knowledge to respond to.
Thanks to Colin, I now do.</p>
</details>
<p>With the data + hypothesis interpretation of Bayes' rule in hand,
the next question arises:
What math happens when we calculate posterior densities?</p>
<h2 id="translating-bayes-math-to-python">Translating Bayes' Math to Python</h2>
<h3 id="defining-posterior-log-likelihood">Defining Posterior Log-Likelihood</h3>
<p>To understand this, let's look at the simplest complex example
that I could think of:
Estimating the <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> parameters
of a normal distribution
conditioned on observing data points <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>.</p>
<p>If we assume a data generating process that looks like the following
(with no probability distributions specified yet):</p>
<div class="mermaid">graph TD;
    μ((μ)) --&gt; y(y);
    σ((σ)) --&gt; y(y);</div>

<p>We can write out the following probabilistic model
(now explicitly specifying probability distributions):</p>
<div>
<div class="MathJax_Preview">\mu \sim Normal(0, 10)</div>
<script type="math/tex; mode=display">\mu \sim Normal(0, 10)</script>
</div>
<div>
<div class="MathJax_Preview">\sigma \sim Exponential(1)</div>
<script type="math/tex; mode=display">\sigma \sim Exponential(1)</script>
</div>
<div>
<div class="MathJax_Preview">y \sim Normal(\mu, \sigma)</div>
<script type="math/tex; mode=display">y \sim Normal(\mu, \sigma)</script>
</div>
<p>Let's now map the symbols onto Bayes' rule.</p>
<ul>
<li><span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> are the parameters, which are <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> here.</li>
<li><span><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> is the data that I will observe</li>
<li><span><span class="MathJax_Preview">P(H|D)</span><script type="math/tex">P(H|D)</script></span> is the posterior, which we would like to compute.</li>
<li><span><span class="MathJax_Preview">P(D|H)</span><script type="math/tex">P(D|H)</script></span> is the likelihood,
and is given by <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>'s probability distribution <span><span class="MathJax_Preview">Normal(\mu, \sigma)</span><script type="math/tex">Normal(\mu, \sigma)</script></span>,
or in probability notation, <span><span class="MathJax_Preview">P(y|\mu, \sigma)</span><script type="math/tex">P(y|\mu, \sigma)</script></span>.</li>
<li><span><span class="MathJax_Preview">P(H)</span><script type="math/tex">P(H)</script></span> is the the prior, and is given by <span><span class="MathJax_Preview">P(\mu, \sigma)</span><script type="math/tex">P(\mu, \sigma)</script></span>.</li>
<li><span><span class="MathJax_Preview">P(D)</span><script type="math/tex">P(D)</script></span> is a hard quantity to calculate, so we sort of cheat and don't use it,
and merely claim that the posterior is proportional to likelihood times prior.</li>
</ul>
<p>If we look at the probability symbols again,
we should notice that <span><span class="MathJax_Preview">P(\mu, \sigma)</span><script type="math/tex">P(\mu, \sigma)</script></span>
is the joint distribution between <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>.
However, from observing the graphical diagram,
we'll notice that <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> have no bearing on one another:
we do not need to know <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> to know the value of <span><span class="MathJax_Preview">sigma</span><script type="math/tex">sigma</script></span>,
and vice versa.
Hence, they are independent of one another,
and so by the rules of probability,</p>
<div>
<div class="MathJax_Preview">P(\mu, \sigma) = P(\mu | \sigma)P(\sigma) = P(\mu)P(\sigma) = P(H)</div>
<script type="math/tex; mode=display">P(\mu, \sigma) = P(\mu | \sigma)P(\sigma) = P(\mu)P(\sigma) = P(H)</script>
</div>
<p>Now, by simply moving symbols around:</p>
<div>
<div class="MathJax_Preview">P(H|D) = P(D|H)P(H)</div>
<script type="math/tex; mode=display">P(H|D) = P(D|H)P(H)</script>
</div>
<div>
<div class="MathJax_Preview"> = P(y|\mu,\sigma)P(\mu, \sigma)</div>
<script type="math/tex; mode=display"> = P(y|\mu,\sigma)P(\mu, \sigma)</script>
</div>
<div>
<div class="MathJax_Preview"> = P(y|\mu, \sigma)P(\mu)P(\sigma)</div>
<script type="math/tex; mode=display"> = P(y|\mu, \sigma)P(\mu)P(\sigma)</script>
</div>
<p>This translates directly into Python code!</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">model_prob</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Probability of mu under prior.</span>
    <span class="n">normal_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">mu_prob</span> <span class="o">=</span> <span class="n">normal_prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># Probability of sigma under prior.</span>
    <span class="n">sigma_prior</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma_prob</span> <span class="o">=</span> <span class="n">sigma_prior</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># Likelihood of data given mu and sigma</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">likelihood_prob</span> <span class="o">=</span> <span class="n">likelihood</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Joint likelihood</span>
    <span class="k">return</span> <span class="n">mu_prob</span> <span class="o">*</span> <span class="n">sigma_prob</span> <span class="o">*</span> <span class="n">likelihood_prob</span>
</pre></div>

<p>If you remember, multiplying so many probability distributions together
can give us underflow issues when computing,
so it is common to take the log of both sides.</p>
<div>
<div class="MathJax_Preview">\log(P(H|D)) = log(P(y|\mu, \sigma)) + log(P(\mu)) + log(P(\sigma))</div>
<script type="math/tex; mode=display">\log(P(H|D)) = log(P(y|\mu, \sigma)) + log(P(\mu)) + log(P(\sigma))</script>
</div>
<p>This also translates directly into Python code!</p>
<div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">model_log_prob</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># log-probability of mu under prior.</span>
    <span class="n">normal_prior</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">mu_log_prob</span> <span class="o">=</span> <span class="n">normal_prior</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># log-probability of sigma under prior.</span>
    <span class="n">sigma_prior</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma_log_prob</span> <span class="o">=</span> <span class="n">sigma_prior</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="c1"># log-likelihood given priors and data</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">likelihood_log_prob</span> <span class="o">=</span> <span class="n">likelihood</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Joint log-likelihood</span>
    <span class="k">return</span> <span class="n">mu_log_prob</span> <span class="o">+</span> <span class="n">sigma_log_prob</span> <span class="o">+</span> <span class="n">likelihood_log_prob</span>
</pre></div>

<h2 id="computing-the-posterior-with-sampling">Computing the Posterior with Sampling</h2>
<p>To identify what the values of <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>
should take on given the data and priors,
we can turn to sampling to help us.
I am intentionally skipping over integrals
which are used to compute expectations,
which is what sampling is replacing.</p>
<h3 id="metropolis-hastings-sampling">Metropolis-Hastings Sampling</h3>
<p>An easy-to-understand sampler that we can start with
is the Metropolis-Hastings sampler.
I first learned it in a grad-level computational biology class,
but I expect most statistics undergrads should have
a good working knowledge of the algorithm.</p>
<p>For the rest of us, check out the note below on how the algorithm works.</p>
<details class="note" open="open"><summary>The Metropolis-Hastings Algorithm</summary><p>Shamelessly copied (and modified)
from the <a href="">Wikipedia article</a>:</p>
<ul>
<li>For each parameter <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>, do the following.</li>
<li>Initialize an arbitrary point for the parameter (this is <span><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>, or <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> at step <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>).</li>
<li>Define a probability density <span><span class="MathJax_Preview">P(p_t)</span><script type="math/tex">P(p_t)</script></span>, for which we will draw new values of the parameters. Here, we will use <span><span class="MathJax_Preview">P(p) = Normal(p_{t-1}, 1)</span><script type="math/tex">P(p) = Normal(p_{t-1}, 1)</script></span>.</li>
<li>For each iteration:<ul>
<li>Generate candidate new candidate <span><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span> drawn from <span><span class="MathJax_Preview">P(p_t)</span><script type="math/tex">P(p_t)</script></span>.</li>
<li>Calculate the likelihood of the data under the previous parameter value(s) <span><span class="MathJax_Preview">p_{t-1}</span><script type="math/tex">p_{t-1}</script></span>: <span><span class="MathJax_Preview">L(p_{t-1})</span><script type="math/tex">L(p_{t-1})</script></span></li>
<li>Calculate the likelihood of the data under the proposed parameter value(s) <span><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>: <span><span class="MathJax_Preview">L(p_t)</span><script type="math/tex">L(p_t)</script></span></li>
<li>Calculate acceptance ratio <span><span class="MathJax_Preview">r = \frac{L(p_t)}{L(p_{t-1})}</span><script type="math/tex">r = \frac{L(p_t)}{L(p_{t-1})}</script></span>.</li>
<li>Generate a new random number on the unit interval: <span><span class="MathJax_Preview">s \sim U(0, 1)</span><script type="math/tex">s \sim U(0, 1)</script></span>.</li>
<li>Compare <span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> to <span><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span>.<ul>
<li>If <span><span class="MathJax_Preview">s \leq r</span><script type="math/tex">s \leq r</script></span>, accept <span><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span>.</li>
<li>If <span><span class="MathJax_Preview">s \gt r</span><script type="math/tex">s \gt r</script></span>, reject <span><span class="MathJax_Preview">p_t</span><script type="math/tex">p_t</script></span> and continue sampling again with <span><span class="MathJax_Preview">p_{t-1}</span><script type="math/tex">p_{t-1}</script></span>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>
<p>In the algorithm described in the note above,
our parameters <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> are actually <span><span class="MathJax_Preview">(\mu, \sigma)</span><script type="math/tex">(\mu, \sigma)</script></span>.
This means that we have to propose two numbers
and sample two numbers in each loop of the sampler.</p>
<p>To make things simple for us, let's use the normal distribution
centered on <span><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span> but with scale <span><span class="MathJax_Preview">0.1</span><script type="math/tex">0.1</script></span>
to propose values for each.</p>
<p>We can implement the algorithm in Python code:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="c1"># Metropolis-Hastings Sampling</span>
<span class="n">mu_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span>
<span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">()</span>

<span class="c1"># Keep a history of the parameter values and ratio.</span>
<span class="n">mu_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">sigma_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">ratio_history</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">mu_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mu_prev</span>
    <span class="n">sigma_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma_prev</span>
    <span class="n">mu_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">sigma_prev</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># Compute joint log likelihood</span>
    <span class="n">LL_t</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_t</span><span class="p">,</span> <span class="n">sigma_t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">LL_prev</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="n">sigma_prev</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Calculate the difference in log-likelihoods</span>
    <span class="c1"># (or a.k.a. ratio of likelihoods)</span>
    <span class="n">diff_log_like</span> <span class="o">=</span> <span class="n">LL_t</span> <span class="o">-</span> <span class="n">LL_prev</span>
    <span class="k">if</span> <span class="n">diff_log_like</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># We need to exponentiate to get the correct ratio,</span>
        <span class="c1"># since all of our calculations were in log-space</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">diff_log_like</span><span class="p">)</span>

    <span class="c1"># Defensive programming check</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">ratio</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;LL_t: {LL_t}, LL_prev: {LL_prev}&quot;</span><span class="p">)</span>

    <span class="c1"># Ratio comparison step</span>
    <span class="n">ratio_history</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">ratio</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ratio</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="p">:</span>
        <span class="n">mu_prev</span> <span class="o">=</span> <span class="n">mu_t</span>
        <span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">sigma_t</span>
</pre></div>
</td></tr></table>

<p>Because of a desire for convenience,
we chose to use a single normal distribution to sample all values.
However, that distribution choice is going to bite us during sampling,
because the values that we could possibly sample for the <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> parameter
can take on negatives,
but when a negative <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> is passed
into the normally-distributed likelihood,
we are going to get computation errors!
This is because the scale parameter of a normal distribution
can only be positive, and cannot be negative or zero.
(If it were zero, there would be no randomness.)</p>
<h3 id="transformations-as-a-hack">Transformations as a Hack</h3>
<p>The key problem here is that the support of the Exponential distribution
is bound to be positive real numbers only.
That said, we can get around this problem
simply by sampling amongst the unbounded real number space <span><span class="MathJax_Preview">(-\inf, +\inf)</span><script type="math/tex">(-\inf, +\inf)</script></span>,
and then transforming the number by a math function to be in the bounded space.</p>
<p>One way we can transform numbers from an unbounded space
to a positive-bounded space
is to use the exponential transform:</p>
<div>
<div class="MathJax_Preview">y = e^x</div>
<script type="math/tex; mode=display">y = e^x</script>
</div>
<p>For any given value <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> will be guaranteed to be positive.</p>
<p>Knowing this, we can modify our sampling code, specifically, what was before:</p>
<div class="codehilite"><pre><span></span><span class="c1"># Initialize in unconstrained space</span>
<span class="n">sigma_prev_unbounded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># ...</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="c1"># Propose in unconstrained space</span>
    <span class="n">sigma_t_unbounded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">sigma_prev</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="c1"># Transform the sampled values to the constrained space</span>
    <span class="n">sigma_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma_prev_unbounded</span><span class="p">)</span>
    <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">sigma_t_unbounded</span><span class="p">)</span>

    <span class="c1"># ...</span>

    <span class="c1"># Pass the transformed values into the log-likelihood calculation</span>
    <span class="n">LL_t</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_t</span><span class="p">,</span> <span class="n">sigma_t</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">LL_prev</span> <span class="o">=</span> <span class="n">model_log_prob</span><span class="p">(</span><span class="n">mu_prev</span><span class="p">,</span> <span class="n">sigma_prev</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># ...</span>
</pre></div>

<p>And <em>voila</em>!
If you notice, the key trick here was
to <strong>sample in unbounded space</strong>,
but <strong>evalute log-likelihood in bounded space</strong>.
We call the "unbounded" space the <em>transformed</em> space,
while the "bounded" space is the <em>original</em> or <em>untransformed</em> space.
We have implemented the necessary components
to compute posterior distributions on parameters!</p>
<h3 id="samples-from-posterior">Samples from Posterior</h3>
<p>If we simulate 1000 data points from a <span><span class="MathJax_Preview">Normal(3, 1)</span><script type="math/tex">Normal(3, 1)</script></span> distribution,
and pass them into the model log probability function defined above,
then after running the sampler,
we get a chain of values that the sampler has picked out
as maximizing the joint likelihood of the data and the model.
This, by the way, is essentially the simplest version of
Markov Chain Monte Carlo sampling that exists
in modern computational Bayesian statistics.</p>
<p>Let's examine the trace from one run:</p>
<p><img alt="" src="../comp-bayes-figures/mcmc-trace.png" /></p>
<p>Notice how it takes about 200 steps before the trace becomes <strong>stationary</strong>,
that is it becomes a flat trend-line.
If we prune the trace to just the values after the 200th iteration,
we get the following trace:</p>
<p><img alt="" src="../comp-bayes-figures/mcmc-trace-burn-in.png" /></p>
<p>The samples drawn are an approximation to
the expected values of <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span>
given the data and priors specified.</p>
<details class="note" open="open"><summary>Random Variables and Sampling</summary><p>A piece of wisdom directly quoted from my friend <a href="https://colindcarroll.com/">Colin Carroll</a>,
who is also a PyMC developer:</p>
<blockquote>
<p>Random variables are <em>measures</em>,
and measures are only really defined under an integral sign.
<em>Sampling</em> is usually defined as the act of generating data
according to a certain measure.
This is confusing, because we invert this relationship
when we do computational statistics:
we generate the data,
and use that to approximate an integral or expectation.</p>
</blockquote>
</details>
<h2 id="topics-we-skipped-over">Topics We Skipped Over</h2>
<p>We intentionally skipped over a number of topics.</p>
<p>One of them was why we used a normal distribution with scale of 0.1
to propose a different value, rather than a different scale.
As it turns out the, scale parameter is a tunable hyperparameter,
and in PyMC3 we do perform tuning as well.
If you want to learn more about how tuning happens,
<a href="https://colindcarroll.com/">Colin</a> has a <a href="https://colcarroll.github.io/hmc_tuning_talk/">great essay</a> on that too.</p>
<p>We also skipped over API design,
as that is a topic I will be exploring in a separate essay.
It will also serve as a tour through the PyMC3 API
as I understand it.</p>
<h2 id="an-anchoring-thought-framework-for-learning-computational-bayes">An Anchoring Thought Framework for Learning Computational Bayes</h2>
<p>Having gone through this exercise
has been extremely helpful in deciphering
what goes on behind-the-scenes in PyMC3
(and the in-development PyMC4,
which is built on top of TensorFlow probability).</p>
<p>From digging through everything from scratch,
my thought framework to think about Bayesian modelling
has been updated (pun intended) to the following.</p>
<p>Firstly, we can view a Bayesian model
from the axis of <strong>prior, likelihood, posterior</strong>.
Bayes' rule provides us the equation "glue"
that links those three components together.</p>
<p>Secondly, when doing <em>computational</em> Bayesian statistics,
we should be able to modularly separate <strong>sampling</strong>
from <strong>model definition</strong>.
<strong>Sampling</strong> is computing the posterior distribution of parameters
given the model and data.
<strong>Model definition</strong>, by contrast,
is all about providing the model structure
as well as a function that calculates the joint log likelihood
of the model and data.</p>
<p>In fact, based on the exercise above,
any "sampler" is only concerned with the model log probability
(though some also require the local gradient of the log probability
w.r.t. the parameters to find where to climb next),
and should only be required to accept a <strong>model log probability</strong> function
and a proposed set of initial parameter values,
and return a chain of sampled values.</p>
<p>Finally, I hope the "simplest complex example"
of estimating <span><span class="MathJax_Preview">\mu</span><script type="math/tex">\mu</script></span> and <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> of a normal distribution
helps further your understanding of the math behind Bayesian statistics.</p>
<p>All in all, I hope this essay helps your learning, as writing it did for me!</p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../computing/recursion/" title="Recursion" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Recursion
              </span>
            </div>
          </a>
        
        
          <a href="../message-passing/" title="Computational Representations of Message Passing" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Computational Representations of Message Passing
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../../assets/fonts/font-awesome.css">
    
      <a href="http://www.shortwhale.com/ericmjl" class="md-footer-social__link fa fa-envelope"></a>
    
      <a href="https://github.com/ericmjl" class="md-footer-social__link fa fa-github"></a>
    
      <a href="https://twitter.com/ericmjl" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://linkedin.com/in/ericmjl" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/application.ac79c3b0.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"../.."}})</script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js"></script>
      
    
  </body>
</html>