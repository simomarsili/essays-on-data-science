{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Essays on Data Science In which I put together my thoughts on the practice of data science. This is a curated and edited collection of my blog posts, as well as essays specially written for the broader Python community.","title":"Essays on Data Science"},{"location":"#essays-on-data-science","text":"In which I put together my thoughts on the practice of data science. This is a curated and edited collection of my blog posts, as well as essays specially written for the broader Python community.","title":"Essays on Data Science"},{"location":"computing/recursion/","text":"Recursion Recursion is an incredibly useful concept to know. To be clear, it is distinct from looping, but is related. I think it's helpful for data scientists to have recursion as a programming trick in their back pocket. In this essay, let's take an introductory look at recursion, and where it can come in handy. What recursion looks like Recursion happens when we have a function that calls itself by default or else returns a result when some stopping criteria is reached. A classic example of recursion is in finding the root of a tree from a given node. Here, we essentially want to follow every node's predecessor until we reach a node that has no predecessor. In code form, this looks something like this: 1 2 3 4 5 6 def find_root ( G , n ): predecessor = G . predecessor ( n ) if G . predecessor ( n ): return find_root ( G , predecessor ) else : return n Generally, we first compute something on the basis of the inputs (line 2). This is usually some form of finding a new substitute input on which we can check a condition (lines 4 and 6). Under one condition, we return the function call with a new input, and under another condition, we return the desired output. Why you would use recursion Recursion is essentially a neat way to write a loop concisely, and can be useful, say, under circumstances where we do not know the exact number of loop iterations needed before we encounter the stopping condition. While I do find recursion useful in certain applied settings, I will also clarify that I don't use recursion on a daily basis. As such, I recommend this as a back-pocket trick that one should have, but won't necessarily use all the time. Where recursion shows up in a real-life situation I can speak to one situation at work where I was benchmarking some deep neural network models, and also testing hyperparameters on a grid. There, I used YAML files to keep track of parameters and experiments, and in order to keep things concise, I implemented a very lightweight YAML inheritance scheme, where I would have a master \"template\" experiment, but use child YAML files that inherited from the \"master\" template in which certain parts of the experiment parameters were changed. (An example might be one where the master template specified the use of the Adam optimizer with a particular learning rate, while the child templates simply modified the learning rate.) As the experiments got deeper and varied more parameters, things became more tree-like, and so I had to navigate the parameter tree from the child templates up till the root template, which by definition had no parents. After finding the root template, I could then travel back down from the root template, iteratively updating the parameters until I reached the child template of interest. The more general scenario to look out for is in graph traversal problems. If your problem can be cast in terms of a graph data structure that you need to program your computer to take a walk over, then that is a prime candidate for trying your hand at recursion.","title":"Recursion"},{"location":"computing/recursion/#recursion","text":"Recursion is an incredibly useful concept to know. To be clear, it is distinct from looping, but is related. I think it's helpful for data scientists to have recursion as a programming trick in their back pocket. In this essay, let's take an introductory look at recursion, and where it can come in handy.","title":"Recursion"},{"location":"computing/recursion/#what-recursion-looks-like","text":"Recursion happens when we have a function that calls itself by default or else returns a result when some stopping criteria is reached. A classic example of recursion is in finding the root of a tree from a given node. Here, we essentially want to follow every node's predecessor until we reach a node that has no predecessor. In code form, this looks something like this: 1 2 3 4 5 6 def find_root ( G , n ): predecessor = G . predecessor ( n ) if G . predecessor ( n ): return find_root ( G , predecessor ) else : return n Generally, we first compute something on the basis of the inputs (line 2). This is usually some form of finding a new substitute input on which we can check a condition (lines 4 and 6). Under one condition, we return the function call with a new input, and under another condition, we return the desired output.","title":"What recursion looks like"},{"location":"computing/recursion/#why-you-would-use-recursion","text":"Recursion is essentially a neat way to write a loop concisely, and can be useful, say, under circumstances where we do not know the exact number of loop iterations needed before we encounter the stopping condition. While I do find recursion useful in certain applied settings, I will also clarify that I don't use recursion on a daily basis. As such, I recommend this as a back-pocket trick that one should have, but won't necessarily use all the time.","title":"Why you would use recursion"},{"location":"computing/recursion/#where-recursion-shows-up-in-a-real-life-situation","text":"I can speak to one situation at work where I was benchmarking some deep neural network models, and also testing hyperparameters on a grid. There, I used YAML files to keep track of parameters and experiments, and in order to keep things concise, I implemented a very lightweight YAML inheritance scheme, where I would have a master \"template\" experiment, but use child YAML files that inherited from the \"master\" template in which certain parts of the experiment parameters were changed. (An example might be one where the master template specified the use of the Adam optimizer with a particular learning rate, while the child templates simply modified the learning rate.) As the experiments got deeper and varied more parameters, things became more tree-like, and so I had to navigate the parameter tree from the child templates up till the root template, which by definition had no parents. After finding the root template, I could then travel back down from the root template, iteratively updating the parameters until I reached the child template of interest. The more general scenario to look out for is in graph traversal problems. If your problem can be cast in terms of a graph data structure that you need to program your computer to take a walk over, then that is a prime candidate for trying your hand at recursion.","title":"Where recursion shows up in a real-life situation"},{"location":"machine-learning/computational-bayesian-stats/","text":"An Introduction to Probability and Computational Bayesian Statistics In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here? Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference. Probability Distributions We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\". Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution. Base Object Implementation Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as: class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma Probability Density Function Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability distribution function\", or the \"PDF\". Using this, we may then extend extend this object with a method called .pdf(x) , that returns a number giving the number of credibility points assigned to the value of x passed in. import numpy as np class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma def pdf ( self , x ): return ( 1 / np . sqrt ( 2 * self . sigma ** 2 * np . pi ) * np . exp ( - ( x - self . mu ) ** 2 / 2 * self . sigma ** 2 )) If we pass in a number x from the number line, we will get back another number that tells us the number of credibility points given to that value x , under the state of the normal distribution instantiated. We'll call this P(x) P(x) . To simplify the implementation used here, we are going to borrow some machinery already available to us in the Python scientific computing ecosystem, particularly from the SciPy stats module, which gives us reference implementations of probability distributions. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) Log Probability A common task in Bayesian inference is computing the likelihood of data. Let's assume that the data {X_1, X_2, ... X_i} {X_1, X_2, ... X_i} generated are independent and identically distributed, (the famous i.i.d. term comes from this). This means, then, that the joint probability of the data that was generated is equivalent to the product of the individual probabilities of each datum: P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) (We have to know the rules of probability to know this result; it is a topic for a different essay.) If you remember the notation above, each P(X_i) P(X_i) is an evaluation of X_i X_i on the distribution's probability density function. It being a probability value means it is bound between 0 and 1. However, multiplying many probabilities together usually will result in issues with underflow computationally, so in evaluating likelihoods, we usually stick with log-likelihoods instead. By the usual rules of math, then: \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) To our normal distribution class, we can now add in another class method that computes the sum of log likelihoods evaluated at a bunch of i.i.d. data points. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) def logpdf ( self , x ): return self . dist . logpdf ( x ) Random Variables Definition Informally, a \"random variable\" is nothing more than a variable whose quantity is non-deterministic (hence random) but whose probability of taking on a certain value can be described by a probability distribution. According to the Wikipedia definition of a random variable : A random variable has a probability distribution, which specifies the probability of its values. As such, it may be tempting to conceive of a random variable as an object that has a probability distribution attribute attached to it. Realizations of a Random Variable On the other hand, it can also be convenient to invert that relationship, and claim that a probability distribution can generate realizations of a random variable. The latter is exactly how SciPy distributions are implemented: from scipy.stats import norm # Normal distribution can generate realizations of an RV # The following returns a NumPy array of 10 draws # from a standard normal distribution. norm ( loc = 0 , scale = 1 ) . rvs ( 10 ) Realizations of a Random Variable A \"realization\" of a random variable is nothing more than generating a random number whose probability of being generated is defined by the random variable's probability density function. Because the generation of realizations of a random variable is equivalent to sampling from a probability distribution, we can extend our probability distribution definition to include a .sample(n) method: from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) # ... def sample ( self , n ): return self . dist . rvs ( n ) Now, if we draw 10 realizations of a normally distributed random variable, and the drawing of each realization has no dependence of any kind on the previous draw, then we can claim that each draw is independent and identically distributed . This is where the fabled \" iid \" term in undergraduate statistics classes comes from. Data Generating Process Now that we have covered what probability distributions are, we can now move on to other concepts that are important in Bayesian statistical modelling. Realizations of a random variable, or draws from its probability distribution, are how a Bayesian assumes data are generated. Describing how data are generated using probability distributions, or in other words, writing down the \"data generating process\", is a core activity in Bayesian statistical modelling. Viewed this way, data values generated by a random process depend on the underlying random variable's probability distribution. In other words, the random variable realizations are known, given the probability distribution used to model it. Keep this idea in mind: it is going to be important shortly. Bayes' Rule Now that we've covered probability distributions, we can move on to Bayes' rule. You probably have seen the following equation: P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} Bayes' rule states nothing more than the fact that the conditional probability of B given A is equal to the conditional probability of A given B times the probability of B divided by the probability of A. When doing Bayesian statistical inference, we commonly take a related but distinct interpretation: P(H|D) = \\frac{P(D|H)P(H)}{P(D)} P(H|D) = \\frac{P(D|H)P(H)}{P(D)} It may look weird, but didn't we say before that data are realizations from a random variable? Why are we now treating data as a random variable? Here, we are doing not-so-intuitive but technically correct step of treating the data D D as being part of this probabilistic model (hence it \"looks\" like a random variable), alongside our model parameters H H . There's a lot of measure theory that goes into this interpretation, which at this point I have not yet mastered, and so will wave my hands in great arcs and propose that this interpretation be accepted for now and move on. Data are random variables? Notes from a chat with Colin gave me a lot to chew on, as usual: The answer is in how you define \"event\" as \"an element of a sigma algebra\". intuitively, an \"event\" is just an abstraction, so one event might be \"the coin is heads\", or in another context the event might be \"the parameters are [0.2, 0.1, 0.2]\". And so analogously, \"the data were configured as [0, 5, 2, 3]\". Notice also that the events are different if the data being ordered vs unordered are different! This was a logical leap that I had been asked about before, but did not previously have the knowledge to respond to. Thanks to Colin, I now do. With the data + hypothesis interpretation of Bayes' rule in hand, the next question arises: What math happens when we calculate posterior densities? Translating Bayes' Math to Python Defining Posterior Log-Likelihood To understand this, let's look at the simplest complex example that I could think of: Estimating the \\mu \\mu and \\sigma \\sigma parameters of a normal distribution conditioned on observing data points y y . If we assume a data generating process that looks like the following (with no probability distributions specified yet): graph TD; \u03bc((\u03bc)) --> y(y); \u03c3((\u03c3)) --> y(y); We can write out the following probabilistic model (now explicitly specifying probability distributions): \\mu \\sim Normal(0, 10) \\mu \\sim Normal(0, 10) \\sigma \\sim Exponential(1) \\sigma \\sim Exponential(1) y \\sim Normal(\\mu, \\sigma) y \\sim Normal(\\mu, \\sigma) Let's now map the symbols onto Bayes' rule. H H are the parameters, which are \\mu \\mu and \\sigma \\sigma here. D D is the data that I will observe P(H|D) P(H|D) is the posterior, which we would like to compute. P(D|H) P(D|H) is the likelihood, and is given by y y 's probability distribution Normal(\\mu, \\sigma) Normal(\\mu, \\sigma) , or in probability notation, P(y|\\mu, \\sigma) P(y|\\mu, \\sigma) . P(H) P(H) is the the prior, and is given by P(\\mu, \\sigma) P(\\mu, \\sigma) . P(D) P(D) is a hard quantity to calculate, so we sort of cheat and don't use it, and merely claim that the posterior is proportional to likelihood times prior. If we look at the probability symbols again, we should notice that P(\\mu, \\sigma) P(\\mu, \\sigma) is the joint distribution between \\mu \\mu and \\sigma \\sigma . However, from observing the graphical diagram, we'll notice that \\mu \\mu and \\sigma \\sigma have no bearing on one another: we do not need to know \\mu \\mu to know the value of sigma sigma , and vice versa. Hence, they are independent of one another, and so by the rules of probability, P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) Now, by simply moving symbols around: P(H|D) = P(D|H)P(H) P(H|D) = P(D|H)P(H) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) This translates directly into Python code! def model_prob ( mu , sigma , y ): # Probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_prob = normal_prior . pdf ( mu ) # Probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_prob = sigma_prior . pdf ( mu ) # Likelihood of data given mu and sigma likelihood = Normal ( mu , sigma ) likelihood_prob = likelihood . pdf ( y ) # Joint likelihood return mu_prob * sigma_prob * likelihood_prob If you remember, multiplying so many probability distributions together can give us underflow issues when computing, so it is common to take the log of both sides. \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) This also translates directly into Python code! def model_log_prob ( mu , sigma , y ): # log-probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_log_prob = normal_prior . logpdf ( mu ) # log-probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_log_prob = sigma_prior . logpdf ( mu ) # log-likelihood given priors and data likelihood = Normal ( mu , sigma ) likelihood_log_prob = likelihood . logpdf ( y ) # Joint log-likelihood return mu_log_prob + sigma_log_prob + likelihood_log_prob Computing the Posterior with Sampling To identify what the values of \\mu \\mu and \\sigma \\sigma should take on given the data and priors, we can turn to sampling to help us. I am intentionally skipping over integrals which are used to compute expectations, which is what sampling is replacing. Metropolis-Hastings Sampling An easy-to-understand sampler that we can start with is the Metropolis-Hastings sampler. I first learned it in a grad-level computational biology class, but I expect most statistics undergrads should have a good working knowledge of the algorithm. For the rest of us, check out the note below on how the algorithm works. The Metropolis-Hastings Algorithm Shamelessly copied (and modified) from the Wikipedia article : For each parameter p p , do the following. Initialize an arbitrary point for the parameter (this is p_t p_t , or p p at step t t ). Define a probability density P(p_t) P(p_t) , for which we will draw new values of the parameters. Here, we will use P(p) = Normal(p_{t-1}, 1) P(p) = Normal(p_{t-1}, 1) . For each iteration: Generate candidate new candidate p_t p_t drawn from P(p_t) P(p_t) . Calculate the likelihood of the data under the previous parameter value(s) p_{t-1} p_{t-1} : L(p_{t-1}) L(p_{t-1}) Calculate the likelihood of the data under the proposed parameter value(s) p_t p_t : L(p_t) L(p_t) Calculate acceptance ratio r = \\frac{L(p_t)}{L(p_{t-1})} r = \\frac{L(p_t)}{L(p_{t-1})} . Generate a new random number on the unit interval: s \\sim U(0, 1) s \\sim U(0, 1) . Compare s s to r r . If s \\leq r s \\leq r , accept p_t p_t . If s \\gt r s \\gt r , reject p_t p_t and continue sampling again with p_{t-1} p_{t-1} . In the algorithm described in the note above, our parameters p p are actually (\\mu, \\sigma) (\\mu, \\sigma) . This means that we have to propose two numbers and sample two numbers in each loop of the sampler. To make things simple for us, let's use the normal distribution centered on 0 0 but with scale 0.1 0.1 to propose values for each. We can implement the algorithm in Python code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # Metropolis-Hastings Sampling mu_prev = np . random . normal () sigma_prev = np . random . normal () # Keep a history of the parameter values and ratio. mu_history = dict () sigma_history = dict () ratio_history = dict () for i in range ( 1000 ): mu_history [ i ] = mu_prev sigma_history [ i ] = sigma_prev mu_t = np . random . normal ( mu_prev , 0.1 ) sigma_t = np . random . normal ( sigma_prev , 0.1 ) # Compute joint log likelihood LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # Calculate the difference in log-likelihoods # (or a.k.a. ratio of likelihoods) diff_log_like = LL_t - LL_prev if diff_log_like > 0 : ratio = 1 else : # We need to exponentiate to get the correct ratio, # since all of our calculations were in log-space ratio = np . exp ( diff_log_like ) # Defensive programming check if np . isinf ( ratio ) or np . isnan ( ratio ): raise ValueError ( f \"LL_t: {LL_t}, LL_prev: {LL_prev}\" ) # Ratio comparison step ratio_history [ i ] = ratio p = np . random . uniform ( 0 , 1 ) if ratio >= p : mu_prev = mu_t sigma_prev = sigma_t Because of a desire for convenience, we chose to use a single normal distribution to sample all values. However, that distribution choice is going to bite us during sampling, because the values that we could possibly sample for the \\sigma \\sigma parameter can take on negatives, but when a negative \\sigma \\sigma is passed into the normally-distributed likelihood, we are going to get computation errors! This is because the scale parameter of a normal distribution can only be positive, and cannot be negative or zero. (If it were zero, there would be no randomness.) Transformations as a Hack The key problem here is that the support of the Exponential distribution is bound to be positive real numbers only. That said, we can get around this problem simply by sampling amongst the unbounded real number space (-\\inf, +\\inf) (-\\inf, +\\inf) , and then transforming the number by a math function to be in the bounded space. One way we can transform numbers from an unbounded space to a positive-bounded space is to use the exponential transform: y = e^x y = e^x For any given value x x , y y will be guaranteed to be positive. Knowing this, we can modify our sampling code, specifically, what was before: # Initialize in unconstrained space sigma_prev_unbounded = np . random . normal ( 0 , 1 ) # ... for i in range ( 1000 ): # ... # Propose in unconstrained space sigma_t_unbounded = np . random . normal ( sigma_prev , 0.1 ) # Transform the sampled values to the constrained space sigma_prev = np . exp ( sigma_prev_unbounded ) sigma_t = np . exp ( sigma_t_unbounded ) # ... # Pass the transformed values into the log-likelihood calculation LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # ... And voila ! If you notice, the key trick here was to sample in unbounded space , but evalute log-likelihood in bounded space . We call the \"unbounded\" space the transformed space, while the \"bounded\" space is the original or untransformed space. We have implemented the necessary components to compute posterior distributions on parameters! Samples from Posterior If we simulate 1000 data points from a Normal(3, 1) Normal(3, 1) distribution, and pass them into the model log probability function defined above, then after running the sampler, we get a chain of values that the sampler has picked out as maximizing the joint likelihood of the data and the model. This, by the way, is essentially the simplest version of Markov Chain Monte Carlo sampling that exists in modern computational Bayesian statistics. Let's examine the trace from one run: Notice how it takes about 200 steps before the trace becomes stationary , that is it becomes a flat trend-line. If we prune the trace to just the values after the 200th iteration, we get the following trace: The samples drawn are an approximation to the expected values of \\mu \\mu and \\sigma \\sigma given the data and priors specified. Random Variables and Sampling A piece of wisdom directly quoted from my friend Colin Carroll , who is also a PyMC developer: Random variables are measures , and measures are only really defined under an integral sign. Sampling is usually defined as the act of generating data according to a certain measure. This is confusing, because we invert this relationship when we do computational statistics: we generate the data, and use that to approximate an integral or expectation. Topics We Skipped Over We intentionally skipped over a number of topics. One of them was why we used a normal distribution with scale of 0.1 to propose a different value, rather than a different scale. As it turns out the, scale parameter is a tunable hyperparameter, and in PyMC3 we do perform tuning as well. If you want to learn more about how tuning happens, Colin has a great essay on that too. We also skipped over API design, as that is a topic I will be exploring in a separate essay. It will also serve as a tour through the PyMC3 API as I understand it. An Anchoring Thought Framework for Learning Computational Bayes Having gone through this exercise has been extremely helpful in deciphering what goes on behind-the-scenes in PyMC3 (and the in-development PyMC4, which is built on top of TensorFlow probability). From digging through everything from scratch, my thought framework to think about Bayesian modelling has been updated (pun intended) to the following. Firstly, we can view a Bayesian model from the axis of prior, likelihood, posterior . Bayes' rule provides us the equation \"glue\" that links those three components together. Secondly, when doing computational Bayesian statistics, we should be able to modularly separate sampling from model definition . Sampling is computing the posterior distribution of parameters given the model and data. Model definition , by contrast, is all about providing the model structure as well as a function that calculates the joint log likelihood of the model and data. In fact, based on the exercise above, any \"sampler\" is only concerned with the model log probability (though some also require the local gradient of the log probability w.r.t. the parameters to find where to climb next), and should only be required to accept a model log probability function and a proposed set of initial parameter values, and return a chain of sampled values. Finally, I hope the \"simplest complex example\" of estimating \\mu \\mu and \\sigma \\sigma of a normal distribution helps further your understanding of the math behind Bayesian statistics. All in all, I hope this essay helps your learning, as writing it did for me!","title":"An Introduction to Probability and Computational Bayesian Statistics"},{"location":"machine-learning/computational-bayesian-stats/#an-introduction-to-probability-and-computational-bayesian-statistics","text":"In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here? Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference.","title":"An Introduction to Probability and Computational Bayesian Statistics"},{"location":"machine-learning/computational-bayesian-stats/#probability-distributions","text":"We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\". Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution.","title":"Probability Distributions"},{"location":"machine-learning/computational-bayesian-stats/#base-object-implementation","text":"Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as: class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma","title":"Base Object Implementation"},{"location":"machine-learning/computational-bayesian-stats/#probability-density-function","text":"Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability distribution function\", or the \"PDF\". Using this, we may then extend extend this object with a method called .pdf(x) , that returns a number giving the number of credibility points assigned to the value of x passed in. import numpy as np class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma def pdf ( self , x ): return ( 1 / np . sqrt ( 2 * self . sigma ** 2 * np . pi ) * np . exp ( - ( x - self . mu ) ** 2 / 2 * self . sigma ** 2 )) If we pass in a number x from the number line, we will get back another number that tells us the number of credibility points given to that value x , under the state of the normal distribution instantiated. We'll call this P(x) P(x) . To simplify the implementation used here, we are going to borrow some machinery already available to us in the Python scientific computing ecosystem, particularly from the SciPy stats module, which gives us reference implementations of probability distributions. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x )","title":"Probability Density Function"},{"location":"machine-learning/computational-bayesian-stats/#log-probability","text":"A common task in Bayesian inference is computing the likelihood of data. Let's assume that the data {X_1, X_2, ... X_i} {X_1, X_2, ... X_i} generated are independent and identically distributed, (the famous i.i.d. term comes from this). This means, then, that the joint probability of the data that was generated is equivalent to the product of the individual probabilities of each datum: P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) (We have to know the rules of probability to know this result; it is a topic for a different essay.) If you remember the notation above, each P(X_i) P(X_i) is an evaluation of X_i X_i on the distribution's probability density function. It being a probability value means it is bound between 0 and 1. However, multiplying many probabilities together usually will result in issues with underflow computationally, so in evaluating likelihoods, we usually stick with log-likelihoods instead. By the usual rules of math, then: \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) To our normal distribution class, we can now add in another class method that computes the sum of log likelihoods evaluated at a bunch of i.i.d. data points. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) def logpdf ( self , x ): return self . dist . logpdf ( x )","title":"Log Probability"},{"location":"machine-learning/computational-bayesian-stats/#random-variables","text":"","title":"Random Variables"},{"location":"machine-learning/computational-bayesian-stats/#definition","text":"Informally, a \"random variable\" is nothing more than a variable whose quantity is non-deterministic (hence random) but whose probability of taking on a certain value can be described by a probability distribution. According to the Wikipedia definition of a random variable : A random variable has a probability distribution, which specifies the probability of its values. As such, it may be tempting to conceive of a random variable as an object that has a probability distribution attribute attached to it.","title":"Definition"},{"location":"machine-learning/computational-bayesian-stats/#realizations-of-a-random-variable","text":"On the other hand, it can also be convenient to invert that relationship, and claim that a probability distribution can generate realizations of a random variable. The latter is exactly how SciPy distributions are implemented: from scipy.stats import norm # Normal distribution can generate realizations of an RV # The following returns a NumPy array of 10 draws # from a standard normal distribution. norm ( loc = 0 , scale = 1 ) . rvs ( 10 ) Realizations of a Random Variable A \"realization\" of a random variable is nothing more than generating a random number whose probability of being generated is defined by the random variable's probability density function. Because the generation of realizations of a random variable is equivalent to sampling from a probability distribution, we can extend our probability distribution definition to include a .sample(n) method: from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) # ... def sample ( self , n ): return self . dist . rvs ( n ) Now, if we draw 10 realizations of a normally distributed random variable, and the drawing of each realization has no dependence of any kind on the previous draw, then we can claim that each draw is independent and identically distributed . This is where the fabled \" iid \" term in undergraduate statistics classes comes from.","title":"Realizations of a Random Variable"},{"location":"machine-learning/computational-bayesian-stats/#data-generating-process","text":"Now that we have covered what probability distributions are, we can now move on to other concepts that are important in Bayesian statistical modelling. Realizations of a random variable, or draws from its probability distribution, are how a Bayesian assumes data are generated. Describing how data are generated using probability distributions, or in other words, writing down the \"data generating process\", is a core activity in Bayesian statistical modelling. Viewed this way, data values generated by a random process depend on the underlying random variable's probability distribution. In other words, the random variable realizations are known, given the probability distribution used to model it. Keep this idea in mind: it is going to be important shortly.","title":"Data Generating Process"},{"location":"machine-learning/computational-bayesian-stats/#bayes-rule","text":"Now that we've covered probability distributions, we can move on to Bayes' rule. You probably have seen the following equation: P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} Bayes' rule states nothing more than the fact that the conditional probability of B given A is equal to the conditional probability of A given B times the probability of B divided by the probability of A. When doing Bayesian statistical inference, we commonly take a related but distinct interpretation: P(H|D) = \\frac{P(D|H)P(H)}{P(D)} P(H|D) = \\frac{P(D|H)P(H)}{P(D)} It may look weird, but didn't we say before that data are realizations from a random variable? Why are we now treating data as a random variable? Here, we are doing not-so-intuitive but technically correct step of treating the data D D as being part of this probabilistic model (hence it \"looks\" like a random variable), alongside our model parameters H H . There's a lot of measure theory that goes into this interpretation, which at this point I have not yet mastered, and so will wave my hands in great arcs and propose that this interpretation be accepted for now and move on. Data are random variables? Notes from a chat with Colin gave me a lot to chew on, as usual: The answer is in how you define \"event\" as \"an element of a sigma algebra\". intuitively, an \"event\" is just an abstraction, so one event might be \"the coin is heads\", or in another context the event might be \"the parameters are [0.2, 0.1, 0.2]\". And so analogously, \"the data were configured as [0, 5, 2, 3]\". Notice also that the events are different if the data being ordered vs unordered are different! This was a logical leap that I had been asked about before, but did not previously have the knowledge to respond to. Thanks to Colin, I now do. With the data + hypothesis interpretation of Bayes' rule in hand, the next question arises: What math happens when we calculate posterior densities?","title":"Bayes' Rule"},{"location":"machine-learning/computational-bayesian-stats/#translating-bayes-math-to-python","text":"","title":"Translating Bayes' Math to Python"},{"location":"machine-learning/computational-bayesian-stats/#defining-posterior-log-likelihood","text":"To understand this, let's look at the simplest complex example that I could think of: Estimating the \\mu \\mu and \\sigma \\sigma parameters of a normal distribution conditioned on observing data points y y . If we assume a data generating process that looks like the following (with no probability distributions specified yet): graph TD; \u03bc((\u03bc)) --> y(y); \u03c3((\u03c3)) --> y(y); We can write out the following probabilistic model (now explicitly specifying probability distributions): \\mu \\sim Normal(0, 10) \\mu \\sim Normal(0, 10) \\sigma \\sim Exponential(1) \\sigma \\sim Exponential(1) y \\sim Normal(\\mu, \\sigma) y \\sim Normal(\\mu, \\sigma) Let's now map the symbols onto Bayes' rule. H H are the parameters, which are \\mu \\mu and \\sigma \\sigma here. D D is the data that I will observe P(H|D) P(H|D) is the posterior, which we would like to compute. P(D|H) P(D|H) is the likelihood, and is given by y y 's probability distribution Normal(\\mu, \\sigma) Normal(\\mu, \\sigma) , or in probability notation, P(y|\\mu, \\sigma) P(y|\\mu, \\sigma) . P(H) P(H) is the the prior, and is given by P(\\mu, \\sigma) P(\\mu, \\sigma) . P(D) P(D) is a hard quantity to calculate, so we sort of cheat and don't use it, and merely claim that the posterior is proportional to likelihood times prior. If we look at the probability symbols again, we should notice that P(\\mu, \\sigma) P(\\mu, \\sigma) is the joint distribution between \\mu \\mu and \\sigma \\sigma . However, from observing the graphical diagram, we'll notice that \\mu \\mu and \\sigma \\sigma have no bearing on one another: we do not need to know \\mu \\mu to know the value of sigma sigma , and vice versa. Hence, they are independent of one another, and so by the rules of probability, P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) Now, by simply moving symbols around: P(H|D) = P(D|H)P(H) P(H|D) = P(D|H)P(H) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) This translates directly into Python code! def model_prob ( mu , sigma , y ): # Probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_prob = normal_prior . pdf ( mu ) # Probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_prob = sigma_prior . pdf ( mu ) # Likelihood of data given mu and sigma likelihood = Normal ( mu , sigma ) likelihood_prob = likelihood . pdf ( y ) # Joint likelihood return mu_prob * sigma_prob * likelihood_prob If you remember, multiplying so many probability distributions together can give us underflow issues when computing, so it is common to take the log of both sides. \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) This also translates directly into Python code! def model_log_prob ( mu , sigma , y ): # log-probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_log_prob = normal_prior . logpdf ( mu ) # log-probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_log_prob = sigma_prior . logpdf ( mu ) # log-likelihood given priors and data likelihood = Normal ( mu , sigma ) likelihood_log_prob = likelihood . logpdf ( y ) # Joint log-likelihood return mu_log_prob + sigma_log_prob + likelihood_log_prob","title":"Defining Posterior Log-Likelihood"},{"location":"machine-learning/computational-bayesian-stats/#computing-the-posterior-with-sampling","text":"To identify what the values of \\mu \\mu and \\sigma \\sigma should take on given the data and priors, we can turn to sampling to help us. I am intentionally skipping over integrals which are used to compute expectations, which is what sampling is replacing.","title":"Computing the Posterior with Sampling"},{"location":"machine-learning/computational-bayesian-stats/#metropolis-hastings-sampling","text":"An easy-to-understand sampler that we can start with is the Metropolis-Hastings sampler. I first learned it in a grad-level computational biology class, but I expect most statistics undergrads should have a good working knowledge of the algorithm. For the rest of us, check out the note below on how the algorithm works. The Metropolis-Hastings Algorithm Shamelessly copied (and modified) from the Wikipedia article : For each parameter p p , do the following. Initialize an arbitrary point for the parameter (this is p_t p_t , or p p at step t t ). Define a probability density P(p_t) P(p_t) , for which we will draw new values of the parameters. Here, we will use P(p) = Normal(p_{t-1}, 1) P(p) = Normal(p_{t-1}, 1) . For each iteration: Generate candidate new candidate p_t p_t drawn from P(p_t) P(p_t) . Calculate the likelihood of the data under the previous parameter value(s) p_{t-1} p_{t-1} : L(p_{t-1}) L(p_{t-1}) Calculate the likelihood of the data under the proposed parameter value(s) p_t p_t : L(p_t) L(p_t) Calculate acceptance ratio r = \\frac{L(p_t)}{L(p_{t-1})} r = \\frac{L(p_t)}{L(p_{t-1})} . Generate a new random number on the unit interval: s \\sim U(0, 1) s \\sim U(0, 1) . Compare s s to r r . If s \\leq r s \\leq r , accept p_t p_t . If s \\gt r s \\gt r , reject p_t p_t and continue sampling again with p_{t-1} p_{t-1} . In the algorithm described in the note above, our parameters p p are actually (\\mu, \\sigma) (\\mu, \\sigma) . This means that we have to propose two numbers and sample two numbers in each loop of the sampler. To make things simple for us, let's use the normal distribution centered on 0 0 but with scale 0.1 0.1 to propose values for each. We can implement the algorithm in Python code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # Metropolis-Hastings Sampling mu_prev = np . random . normal () sigma_prev = np . random . normal () # Keep a history of the parameter values and ratio. mu_history = dict () sigma_history = dict () ratio_history = dict () for i in range ( 1000 ): mu_history [ i ] = mu_prev sigma_history [ i ] = sigma_prev mu_t = np . random . normal ( mu_prev , 0.1 ) sigma_t = np . random . normal ( sigma_prev , 0.1 ) # Compute joint log likelihood LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # Calculate the difference in log-likelihoods # (or a.k.a. ratio of likelihoods) diff_log_like = LL_t - LL_prev if diff_log_like > 0 : ratio = 1 else : # We need to exponentiate to get the correct ratio, # since all of our calculations were in log-space ratio = np . exp ( diff_log_like ) # Defensive programming check if np . isinf ( ratio ) or np . isnan ( ratio ): raise ValueError ( f \"LL_t: {LL_t}, LL_prev: {LL_prev}\" ) # Ratio comparison step ratio_history [ i ] = ratio p = np . random . uniform ( 0 , 1 ) if ratio >= p : mu_prev = mu_t sigma_prev = sigma_t Because of a desire for convenience, we chose to use a single normal distribution to sample all values. However, that distribution choice is going to bite us during sampling, because the values that we could possibly sample for the \\sigma \\sigma parameter can take on negatives, but when a negative \\sigma \\sigma is passed into the normally-distributed likelihood, we are going to get computation errors! This is because the scale parameter of a normal distribution can only be positive, and cannot be negative or zero. (If it were zero, there would be no randomness.)","title":"Metropolis-Hastings Sampling"},{"location":"machine-learning/computational-bayesian-stats/#transformations-as-a-hack","text":"The key problem here is that the support of the Exponential distribution is bound to be positive real numbers only. That said, we can get around this problem simply by sampling amongst the unbounded real number space (-\\inf, +\\inf) (-\\inf, +\\inf) , and then transforming the number by a math function to be in the bounded space. One way we can transform numbers from an unbounded space to a positive-bounded space is to use the exponential transform: y = e^x y = e^x For any given value x x , y y will be guaranteed to be positive. Knowing this, we can modify our sampling code, specifically, what was before: # Initialize in unconstrained space sigma_prev_unbounded = np . random . normal ( 0 , 1 ) # ... for i in range ( 1000 ): # ... # Propose in unconstrained space sigma_t_unbounded = np . random . normal ( sigma_prev , 0.1 ) # Transform the sampled values to the constrained space sigma_prev = np . exp ( sigma_prev_unbounded ) sigma_t = np . exp ( sigma_t_unbounded ) # ... # Pass the transformed values into the log-likelihood calculation LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # ... And voila ! If you notice, the key trick here was to sample in unbounded space , but evalute log-likelihood in bounded space . We call the \"unbounded\" space the transformed space, while the \"bounded\" space is the original or untransformed space. We have implemented the necessary components to compute posterior distributions on parameters!","title":"Transformations as a Hack"},{"location":"machine-learning/computational-bayesian-stats/#samples-from-posterior","text":"If we simulate 1000 data points from a Normal(3, 1) Normal(3, 1) distribution, and pass them into the model log probability function defined above, then after running the sampler, we get a chain of values that the sampler has picked out as maximizing the joint likelihood of the data and the model. This, by the way, is essentially the simplest version of Markov Chain Monte Carlo sampling that exists in modern computational Bayesian statistics. Let's examine the trace from one run: Notice how it takes about 200 steps before the trace becomes stationary , that is it becomes a flat trend-line. If we prune the trace to just the values after the 200th iteration, we get the following trace: The samples drawn are an approximation to the expected values of \\mu \\mu and \\sigma \\sigma given the data and priors specified. Random Variables and Sampling A piece of wisdom directly quoted from my friend Colin Carroll , who is also a PyMC developer: Random variables are measures , and measures are only really defined under an integral sign. Sampling is usually defined as the act of generating data according to a certain measure. This is confusing, because we invert this relationship when we do computational statistics: we generate the data, and use that to approximate an integral or expectation.","title":"Samples from Posterior"},{"location":"machine-learning/computational-bayesian-stats/#topics-we-skipped-over","text":"We intentionally skipped over a number of topics. One of them was why we used a normal distribution with scale of 0.1 to propose a different value, rather than a different scale. As it turns out the, scale parameter is a tunable hyperparameter, and in PyMC3 we do perform tuning as well. If you want to learn more about how tuning happens, Colin has a great essay on that too. We also skipped over API design, as that is a topic I will be exploring in a separate essay. It will also serve as a tour through the PyMC3 API as I understand it.","title":"Topics We Skipped Over"},{"location":"machine-learning/computational-bayesian-stats/#an-anchoring-thought-framework-for-learning-computational-bayes","text":"Having gone through this exercise has been extremely helpful in deciphering what goes on behind-the-scenes in PyMC3 (and the in-development PyMC4, which is built on top of TensorFlow probability). From digging through everything from scratch, my thought framework to think about Bayesian modelling has been updated (pun intended) to the following. Firstly, we can view a Bayesian model from the axis of prior, likelihood, posterior . Bayes' rule provides us the equation \"glue\" that links those three components together. Secondly, when doing computational Bayesian statistics, we should be able to modularly separate sampling from model definition . Sampling is computing the posterior distribution of parameters given the model and data. Model definition , by contrast, is all about providing the model structure as well as a function that calculates the joint log likelihood of the model and data. In fact, based on the exercise above, any \"sampler\" is only concerned with the model log probability (though some also require the local gradient of the log probability w.r.t. the parameters to find where to climb next), and should only be required to accept a model log probability function and a proposed set of initial parameter values, and return a chain of sampled values. Finally, I hope the \"simplest complex example\" of estimating \\mu \\mu and \\sigma \\sigma of a normal distribution helps further your understanding of the math behind Bayesian statistics. All in all, I hope this essay helps your learning, as writing it did for me!","title":"An Anchoring Thought Framework for Learning Computational Bayes"},{"location":"machine-learning/message-passing/","text":"Computational Representations of Message Passing Abstract: Message passing on graphs, also known as graph convolutions, have become a popular research topic. In this piece, I aim to provide a short technical primer on ways to implement message passing on graphs. The goal is to provide clear pedagogy on what message passing means mathematically, and hopefully point towards cleaner computational implementations of the key algorithmic pieces. Assumed knowledge: We assume our reader has familiarity with elementary graph concepts. More specifically, the terms \u201cgraph\u201d, \u201cnodes\u201d, and \u201cedges\u201d should be familiar terms. Code examples in this technical piece will be written using the Python programming language, specifically using Python 3.7, NumPy 1.17 (in JAX), and NetworkX 2.2. Introduction to Message Passing Functions on Nodes Message passing starts with a \u201cfunction defined over nodes\u201d, which we will denote here as f(v) f(v) (for \u201cfunction of node/vertex v\u201d). What is this, one might ask? In short, this is nothing more than a numeric value of some kind attached to every node in a graph. This value could be scalar, vector, matrix, or tensor. The semantic meaning of that value is typically defined by the application domain that the graph is being used in. As a concrete example, in molecules, a \u201cfunction\u201d defined over the molecular graph could be the scalar-valued proton number. Carbon would be represented by the function f(v) = 6 f(v) = 6 . Alternatively, it could be a vector of values encompassing both the atomic mass and the number of valence electrons. In this case, carbon would be represented by the function f(v) = (6, 4) f(v) = (6, 4) . Visually, one might represent it as follows: Message Passing What then is message passing, or, as the deep learning community has adopted, \u201cgraph convolution\u201d? At its core, message passing is nothing more than a generic mathematical operation defined between a node\u2019s function value and its neighbors function value. As an example, one may define a message passing operation to be the summation the function evaluated at a node with the function evaluated on its neighbor\u2019s nodes. Here is a simplistic example, shown using a scalar on water: Summation is not the only message passing operation that can be defined. In principle, given any node (or vertex) v v and its neighbors N(v) N(v) values, we may write down a generic function f(v, N(v)) f(v, N(v)) that defines how the function value on each node is to be shared with its neighbors. Computational Implementations of Message Passing For simplicity, let us stay with the particular case where the message passing operation is defined as the summation of one\u2019s neighbors values with one\u2019s values. Object-Oriented Implementation With this definition in place, we may then define a message passing operation in Python as follows: 1 2 3 4 5 6 7 8 9 10 11 12 def message_passing ( G ): \"\"\"Object-oriented message passing operation.\"\"\" G_new = G . copy () for node , data in G . nodes ( data = True ): new_value = data [ \"value\" ] # assuming the value is stored under this key neighbors = G . neighbors ( node ) for neighbor in neighbors : new_value += G . nodes [ neighbor ][ \"value\" ] G_new . node [ node ][ \"value\" ] = new_value return G Thinking about computational considerations, we would naturally consider this implementation to be slow, because it involves a for-loop over Python objects. If we had multiple graphs over which we wanted message passing to be performed, the type-checking overhead in Python will naturally accumulate, and may even dominate. Linear Algebra Implementation How might we speed things up? As it turns out, linear algebra may be useful. We know that every graph may be represented as an adjacency matrix A , whose shape is (n_nodes, n_nodes) . As long as we maintain proper node ordering, we may also define a compatibly-shaped matrix F for node function values, whose shape is (n_nodes, n_features) . Taking advantage of this, in order define the \u201cself plus neighbors\u201d message passing operation in terms of linear algebra operations, we may then modify A by adding to it a diagonal matrix of ones. (In graph terminology, this is equivalent to adding a self-loop to the adjacency matrix.) Then, message passing, as defined above, is trivially the dot product of A and F : 1 2 3 4 5 6 7 8 9 def message_passing ( A , F ): \"\"\" Message passing done by linear algebra. :param A: Adjacency-like matrix, whose shape is (n_nodes, n_nodes). :param F: Feature matrix, whose shape is (n_nodes, n_features). \"\"\" return np . dot ( A , F ) In principle, variants on the adjacency matrix are possible. The only hard requirement for the matrix A is that it has the shape (n_nodes, n_nodes) . Adjacency Variant 1: N-degree adjacency matrix The adjacency matrix represents connectivity by degree 1. If we take the second matrix power of the adjacency matrix, we get back the connectivity of nodes at two degrees of separation away. More generically: 1 2 3 4 5 6 7 8 def n_degree_adjacency ( A , n : int ): \"\"\" Return the n-degree of separation adjacency matrix. :param A: Adjacency matrix, of shape (n_nodes, n_nodes) :param n: Number of degrees of separation. \"\"\" return np . linalg . matrix_power ( A , n ) Performing message passing using the N-degree adjacency matrix effectively describes sharing of information between nodes that are N-degrees of separation apart, skipping intermediate neighbors. Adjacency Variant 2: Graph laplacian matrix The graph laplacian matrix is defined as the diagonal degree matrix D (where the diagonal entries are the degree of each node) minus the adjacency matrix A : L = D - A . This matrix is the discrete analog to the Laplacian operator, and can give us information about the discrete gradient between a node and its neighbors. Message Passing on Multiple Graphs Thus far, we have seen an efficient implementation of message passing on a single graph using linear algebra. How would one perform message passing on multiple graphs, though? This is a question that has applications in graph neural networks (especially in cheminformatics). For the learning task where one has a batch of graphs, and the supervised learning task is to predict a scalar (or vector) value per graph, knowing how to efficiently message pass over multiple graphs is crucial to developing a performant graph neural network model. The challenge here, though, is that graphs generally are of variable size, hence it is not immediately obvious how to \u201ctensorify\u201d the operations. Let us look at a few alternatives, starting with the most obvious (but also most inefficient), building towards more efficient solutions. Implementation 1: For-loops over pairs of adjacency and feature matrices If we multiple graphs, they may be represented as a list of feature matrices and a list of adjacency matrices. The message passing operation, then, may be defined by writing a for-loop over pairs of these matrices. 1 2 3 4 5 def message_passing ( As , Fs ): outputs = [] for A , F in zip ( As , Fs ): outputs . append ( np . dot ( A , F )) return outputs Because of the for-loop, the obvious downside here is the overhead induced by running a for-loop over pairs of As and Fs. Implementation 2: Sparse Matrices Sparse matrices are an attractive alternative. Instead of treating graphs as independent samples, we may treat them as a single large graph on which we perform message passing. If we order the nodes in our adjacency matrix and feature matrix correctly, we will end up with a block diagonal adjacency matrix, and vertically stacked feature matrices. If we prepare the multiple graphs as a large disconnected graph, then we will have a dense feature matrix of shape (sum(n_nodes), n_feats) , and a sparse adjacency matrix of shape (sum(n_nodes), sum(n_nodes)) . Message passing then becomes a sparse-dense dot product: 1 2 def message_passing ( A , F ): return sparse . dot ( A , F ) The upside here is that message passing has been returned back to its natural form (a dot product). The downsides here are that the data must be prepared as a single large graph, hence we effectively lose what one would call the \u201csample\u201d (or \u201cbatch\u201d) dimension. Additionally, the most widely used deep learning libraries do not support automatic differentiation on sparse-dense or dense-sparse dot products, hence limiting the use of this implementation in deep learning. Implementation 3: Size-batched matrix multiplication An alternative way to conceptualize message passing is to think of graphs of the same size as belonging to a \u201csize batch\u201d. We may then vertically stack the feature and adjacency matrices of graphs of the same size together, and perform a batched matrix multiplication, ensuring that we preserve the sample/batch dimension in the final result. In terms of Python code, this requires special preparation of the graphs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from collections import defaultdict from jax.lax import batch_matmul def feature_matrix ( G ): # ... return F def prep_data ( Gs : list ): adjacency_matrices = defaultdict ( list ) feature_matrices = defaultdict ( list ) for G in Gs : size = len ( G ) F = feature_matrix ( G ) A = nx . adjacency_matrix ( G ) + np . ones ( size ) adjacency_matrices [ size ] . append ( A ) feature_matrices [ size ] . append ( A ) for size , As in adjacency_matrices . items (): adjacency_matrices [ size ] = np . stack ( As ) for size , Fs in feature_matrices . items (): feature_matrices [ size ] = np . stack ( Fs ) return adjacency_matrices , feature_matrices def message_passing ( As , Fs ): result = dict () for size in As . keys (): F = Fs [ size ] A = As [ size ] result [ size ] = batch_matmul ( A , F ) return result In this implementation, we use jax.lax.batch_matmul , which inherently assumes that the first dimension is the sample/batch dimension, and that the matrix multiplication happens on the subsequent dimensions. An advantage here is that the number of loop overhead calls in Python is reduced to the number of unique graph sizes that are present in the graph. The disadvantage, though, is that we have a dictionary data structure that we have to deal with, which makes data handling in Python less natural when dealing with linear algebra libraries. Implementation 4: Batched padded matrix multiplication In this implementation, we prepare the data in a different way. Firstly, we must know the size of the largest graph ahead-of-time. 1 size = ... # largest graph size We then pad every graph\u2019s feature matrix with zeros along the node axis until the node axis is as long as the largest graph size. 1 2 3 4 5 6 7 8 9 def prep_feats ( F , size ): # F is of shape (n_nodes, n_feats) return np . pad ( F , [ ( 0 , size - F . shape [ 0 ]), ( 0 , 0 ) ], ) We do the same with every adjacency matrix. 1 2 3 4 5 6 7 8 9 def prep_adjs ( A , size ): # A is of shape (n_nodes, n_nodes) return np . pad ( A , [ ( 0 , size - A . shape [ 0 ]), ( 0 , size - A . shape [ 0 ]), ], ) Finally, we simply stack them into the data matrix: 1 2 As = np . stack ([ prep_adjs ( A , size ) for A in As ] Fs = np . stack ([ prep_feats ( F , size ) for F in Fs ] Now, the shapes of our matrices are as follows: F takes on the shape (n_graphs, n_nodes, n_feats) A takes on the shape (n_graphs, n_nodes, n_nodes) If we desire to be semantically consistent with our shapes, then we might, by convention, assign the first dimension to be the sample/batch dimension. Finally, message passing is now trivially defined as a batch matrix multiply: 1 2 def message_passing ( A , F ): return batch_matmul ( A , F ) Visually, this is represented as follows: To this author\u2019s best knowledge, this should be the most efficient implementation of batched message passing across multiple graphs that also supports automatic differentiation, while also maintaining parity with the written equation form, hence preserving readability. The problems associated with a for-loop, sparse matrix multiplication, and dictionary carries, are removed. Moreover, the sample/batch dimension is preserved, hence it is semantically easy to map each graph to its corresponding output value. Given the current state of automatic differentiation libraries, no additional machinery is necessary to support sparse matrix products. The only disadvantage that this author can think of is that zero-padding may not be intuitive at first glance, and that the data must still be specially prepared and stacked first. Concluding Words This essay was initially motivated by the myriad of difficult-to-read message passing implementations present in the deep learning literature. Frequently, a for-loop of some kind is invoked, or an undocumented list data structure is created, in order to accomplish the message passing operation. Moreover, the model implementation is frequently not separated from the data preparation step, which makes for convoluted and mutually incompatible implementations of message passing in neural networks. It is my hope that while the research field is still in vogue, a technical piece that advises researchers on easily-readable and efficient implementations of message passing on graphs may help advance research practice. In particular, if our code can more closely match the equations listed in papers, that will help facilitate communication and verification of model implementations. To help researchers get started, an example implementation for the full data preparation and batched padded matrix multiplies in JAX is available on GitHub, archived on Zenodo. Acknowledgments I thank Rif. A. Saurous for our discussion at the PyMC4 developer summit in Montreal, QC, where his laser-like focus on \u201ctensorify everything\u201d inspired many new thoughts in my mind. Many thanks to my wife, Nan Li, who first pointed me to the linear algebra equivalents of graphs. I also thank David Duvenaud and Matthew J. Johnson for their pedagogy while they were at Harvard. Appendix Equivalence between padded and non-padded message passing To readers who may need an example to be convinced that matrix multiplying the padded matrices is equivalent to matrix multiplying the originals, we show the Python example below. Firstly, without padding: F = np . array ([[ 1 , 0 ], [ 1 , 1 ]]) A = np . array ([[ 1 , 0 ], [ 0 , 1 ]]) M = np . dot ( A , F ) # Value of M # DeviceArray([[1, 0], # [1, 1]], dtype=int32) And now, with padding: pad_size = 2 F_pad = np . pad ( F , pad_width = [ ( 0 , pad_size ), ( 0 , 0 ), ] ) A_pad = np . pad ( A , pad_width = [ ( 0 , pad_size ), ( 0 , pad_size ), ] ) # F_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32) # A_pad: # DeviceArray([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 0], # [0, 0, 0, 0]], dtype=int32) M_pad = np . dot ( A_pad , F_pad ) # M_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32)","title":"Computational Representations of Message Passing"},{"location":"machine-learning/message-passing/#computational-representations-of-message-passing","text":"Abstract: Message passing on graphs, also known as graph convolutions, have become a popular research topic. In this piece, I aim to provide a short technical primer on ways to implement message passing on graphs. The goal is to provide clear pedagogy on what message passing means mathematically, and hopefully point towards cleaner computational implementations of the key algorithmic pieces. Assumed knowledge: We assume our reader has familiarity with elementary graph concepts. More specifically, the terms \u201cgraph\u201d, \u201cnodes\u201d, and \u201cedges\u201d should be familiar terms. Code examples in this technical piece will be written using the Python programming language, specifically using Python 3.7, NumPy 1.17 (in JAX), and NetworkX 2.2.","title":"Computational Representations of Message Passing"},{"location":"machine-learning/message-passing/#introduction-to-message-passing","text":"","title":"Introduction to Message Passing"},{"location":"machine-learning/message-passing/#functions-on-nodes","text":"Message passing starts with a \u201cfunction defined over nodes\u201d, which we will denote here as f(v) f(v) (for \u201cfunction of node/vertex v\u201d). What is this, one might ask? In short, this is nothing more than a numeric value of some kind attached to every node in a graph. This value could be scalar, vector, matrix, or tensor. The semantic meaning of that value is typically defined by the application domain that the graph is being used in. As a concrete example, in molecules, a \u201cfunction\u201d defined over the molecular graph could be the scalar-valued proton number. Carbon would be represented by the function f(v) = 6 f(v) = 6 . Alternatively, it could be a vector of values encompassing both the atomic mass and the number of valence electrons. In this case, carbon would be represented by the function f(v) = (6, 4) f(v) = (6, 4) . Visually, one might represent it as follows:","title":"Functions on Nodes"},{"location":"machine-learning/message-passing/#message-passing","text":"What then is message passing, or, as the deep learning community has adopted, \u201cgraph convolution\u201d? At its core, message passing is nothing more than a generic mathematical operation defined between a node\u2019s function value and its neighbors function value. As an example, one may define a message passing operation to be the summation the function evaluated at a node with the function evaluated on its neighbor\u2019s nodes. Here is a simplistic example, shown using a scalar on water: Summation is not the only message passing operation that can be defined. In principle, given any node (or vertex) v v and its neighbors N(v) N(v) values, we may write down a generic function f(v, N(v)) f(v, N(v)) that defines how the function value on each node is to be shared with its neighbors.","title":"Message Passing"},{"location":"machine-learning/message-passing/#computational-implementations-of-message-passing","text":"For simplicity, let us stay with the particular case where the message passing operation is defined as the summation of one\u2019s neighbors values with one\u2019s values.","title":"Computational Implementations of Message Passing"},{"location":"machine-learning/message-passing/#object-oriented-implementation","text":"With this definition in place, we may then define a message passing operation in Python as follows: 1 2 3 4 5 6 7 8 9 10 11 12 def message_passing ( G ): \"\"\"Object-oriented message passing operation.\"\"\" G_new = G . copy () for node , data in G . nodes ( data = True ): new_value = data [ \"value\" ] # assuming the value is stored under this key neighbors = G . neighbors ( node ) for neighbor in neighbors : new_value += G . nodes [ neighbor ][ \"value\" ] G_new . node [ node ][ \"value\" ] = new_value return G Thinking about computational considerations, we would naturally consider this implementation to be slow, because it involves a for-loop over Python objects. If we had multiple graphs over which we wanted message passing to be performed, the type-checking overhead in Python will naturally accumulate, and may even dominate.","title":"Object-Oriented Implementation"},{"location":"machine-learning/message-passing/#linear-algebra-implementation","text":"How might we speed things up? As it turns out, linear algebra may be useful. We know that every graph may be represented as an adjacency matrix A , whose shape is (n_nodes, n_nodes) . As long as we maintain proper node ordering, we may also define a compatibly-shaped matrix F for node function values, whose shape is (n_nodes, n_features) . Taking advantage of this, in order define the \u201cself plus neighbors\u201d message passing operation in terms of linear algebra operations, we may then modify A by adding to it a diagonal matrix of ones. (In graph terminology, this is equivalent to adding a self-loop to the adjacency matrix.) Then, message passing, as defined above, is trivially the dot product of A and F : 1 2 3 4 5 6 7 8 9 def message_passing ( A , F ): \"\"\" Message passing done by linear algebra. :param A: Adjacency-like matrix, whose shape is (n_nodes, n_nodes). :param F: Feature matrix, whose shape is (n_nodes, n_features). \"\"\" return np . dot ( A , F ) In principle, variants on the adjacency matrix are possible. The only hard requirement for the matrix A is that it has the shape (n_nodes, n_nodes) .","title":"Linear Algebra Implementation"},{"location":"machine-learning/message-passing/#adjacency-variant-1-n-degree-adjacency-matrix","text":"The adjacency matrix represents connectivity by degree 1. If we take the second matrix power of the adjacency matrix, we get back the connectivity of nodes at two degrees of separation away. More generically: 1 2 3 4 5 6 7 8 def n_degree_adjacency ( A , n : int ): \"\"\" Return the n-degree of separation adjacency matrix. :param A: Adjacency matrix, of shape (n_nodes, n_nodes) :param n: Number of degrees of separation. \"\"\" return np . linalg . matrix_power ( A , n ) Performing message passing using the N-degree adjacency matrix effectively describes sharing of information between nodes that are N-degrees of separation apart, skipping intermediate neighbors.","title":"Adjacency Variant 1: N-degree adjacency matrix"},{"location":"machine-learning/message-passing/#adjacency-variant-2-graph-laplacian-matrix","text":"The graph laplacian matrix is defined as the diagonal degree matrix D (where the diagonal entries are the degree of each node) minus the adjacency matrix A : L = D - A . This matrix is the discrete analog to the Laplacian operator, and can give us information about the discrete gradient between a node and its neighbors.","title":"Adjacency Variant 2: Graph laplacian matrix"},{"location":"machine-learning/message-passing/#message-passing-on-multiple-graphs","text":"Thus far, we have seen an efficient implementation of message passing on a single graph using linear algebra. How would one perform message passing on multiple graphs, though? This is a question that has applications in graph neural networks (especially in cheminformatics). For the learning task where one has a batch of graphs, and the supervised learning task is to predict a scalar (or vector) value per graph, knowing how to efficiently message pass over multiple graphs is crucial to developing a performant graph neural network model. The challenge here, though, is that graphs generally are of variable size, hence it is not immediately obvious how to \u201ctensorify\u201d the operations. Let us look at a few alternatives, starting with the most obvious (but also most inefficient), building towards more efficient solutions.","title":"Message Passing on Multiple Graphs"},{"location":"machine-learning/message-passing/#implementation-1-for-loops-over-pairs-of-adjacency-and-feature-matrices","text":"If we multiple graphs, they may be represented as a list of feature matrices and a list of adjacency matrices. The message passing operation, then, may be defined by writing a for-loop over pairs of these matrices. 1 2 3 4 5 def message_passing ( As , Fs ): outputs = [] for A , F in zip ( As , Fs ): outputs . append ( np . dot ( A , F )) return outputs Because of the for-loop, the obvious downside here is the overhead induced by running a for-loop over pairs of As and Fs.","title":"Implementation 1: For-loops over pairs of adjacency and feature matrices"},{"location":"machine-learning/message-passing/#implementation-2-sparse-matrices","text":"Sparse matrices are an attractive alternative. Instead of treating graphs as independent samples, we may treat them as a single large graph on which we perform message passing. If we order the nodes in our adjacency matrix and feature matrix correctly, we will end up with a block diagonal adjacency matrix, and vertically stacked feature matrices. If we prepare the multiple graphs as a large disconnected graph, then we will have a dense feature matrix of shape (sum(n_nodes), n_feats) , and a sparse adjacency matrix of shape (sum(n_nodes), sum(n_nodes)) . Message passing then becomes a sparse-dense dot product: 1 2 def message_passing ( A , F ): return sparse . dot ( A , F ) The upside here is that message passing has been returned back to its natural form (a dot product). The downsides here are that the data must be prepared as a single large graph, hence we effectively lose what one would call the \u201csample\u201d (or \u201cbatch\u201d) dimension. Additionally, the most widely used deep learning libraries do not support automatic differentiation on sparse-dense or dense-sparse dot products, hence limiting the use of this implementation in deep learning.","title":"Implementation 2: Sparse Matrices"},{"location":"machine-learning/message-passing/#implementation-3-size-batched-matrix-multiplication","text":"An alternative way to conceptualize message passing is to think of graphs of the same size as belonging to a \u201csize batch\u201d. We may then vertically stack the feature and adjacency matrices of graphs of the same size together, and perform a batched matrix multiplication, ensuring that we preserve the sample/batch dimension in the final result. In terms of Python code, this requires special preparation of the graphs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from collections import defaultdict from jax.lax import batch_matmul def feature_matrix ( G ): # ... return F def prep_data ( Gs : list ): adjacency_matrices = defaultdict ( list ) feature_matrices = defaultdict ( list ) for G in Gs : size = len ( G ) F = feature_matrix ( G ) A = nx . adjacency_matrix ( G ) + np . ones ( size ) adjacency_matrices [ size ] . append ( A ) feature_matrices [ size ] . append ( A ) for size , As in adjacency_matrices . items (): adjacency_matrices [ size ] = np . stack ( As ) for size , Fs in feature_matrices . items (): feature_matrices [ size ] = np . stack ( Fs ) return adjacency_matrices , feature_matrices def message_passing ( As , Fs ): result = dict () for size in As . keys (): F = Fs [ size ] A = As [ size ] result [ size ] = batch_matmul ( A , F ) return result In this implementation, we use jax.lax.batch_matmul , which inherently assumes that the first dimension is the sample/batch dimension, and that the matrix multiplication happens on the subsequent dimensions. An advantage here is that the number of loop overhead calls in Python is reduced to the number of unique graph sizes that are present in the graph. The disadvantage, though, is that we have a dictionary data structure that we have to deal with, which makes data handling in Python less natural when dealing with linear algebra libraries.","title":"Implementation 3: Size-batched matrix multiplication"},{"location":"machine-learning/message-passing/#implementation-4-batched-padded-matrix-multiplication","text":"In this implementation, we prepare the data in a different way. Firstly, we must know the size of the largest graph ahead-of-time. 1 size = ... # largest graph size We then pad every graph\u2019s feature matrix with zeros along the node axis until the node axis is as long as the largest graph size. 1 2 3 4 5 6 7 8 9 def prep_feats ( F , size ): # F is of shape (n_nodes, n_feats) return np . pad ( F , [ ( 0 , size - F . shape [ 0 ]), ( 0 , 0 ) ], ) We do the same with every adjacency matrix. 1 2 3 4 5 6 7 8 9 def prep_adjs ( A , size ): # A is of shape (n_nodes, n_nodes) return np . pad ( A , [ ( 0 , size - A . shape [ 0 ]), ( 0 , size - A . shape [ 0 ]), ], ) Finally, we simply stack them into the data matrix: 1 2 As = np . stack ([ prep_adjs ( A , size ) for A in As ] Fs = np . stack ([ prep_feats ( F , size ) for F in Fs ] Now, the shapes of our matrices are as follows: F takes on the shape (n_graphs, n_nodes, n_feats) A takes on the shape (n_graphs, n_nodes, n_nodes) If we desire to be semantically consistent with our shapes, then we might, by convention, assign the first dimension to be the sample/batch dimension. Finally, message passing is now trivially defined as a batch matrix multiply: 1 2 def message_passing ( A , F ): return batch_matmul ( A , F ) Visually, this is represented as follows: To this author\u2019s best knowledge, this should be the most efficient implementation of batched message passing across multiple graphs that also supports automatic differentiation, while also maintaining parity with the written equation form, hence preserving readability. The problems associated with a for-loop, sparse matrix multiplication, and dictionary carries, are removed. Moreover, the sample/batch dimension is preserved, hence it is semantically easy to map each graph to its corresponding output value. Given the current state of automatic differentiation libraries, no additional machinery is necessary to support sparse matrix products. The only disadvantage that this author can think of is that zero-padding may not be intuitive at first glance, and that the data must still be specially prepared and stacked first.","title":"Implementation 4: Batched padded matrix multiplication"},{"location":"machine-learning/message-passing/#concluding-words","text":"This essay was initially motivated by the myriad of difficult-to-read message passing implementations present in the deep learning literature. Frequently, a for-loop of some kind is invoked, or an undocumented list data structure is created, in order to accomplish the message passing operation. Moreover, the model implementation is frequently not separated from the data preparation step, which makes for convoluted and mutually incompatible implementations of message passing in neural networks. It is my hope that while the research field is still in vogue, a technical piece that advises researchers on easily-readable and efficient implementations of message passing on graphs may help advance research practice. In particular, if our code can more closely match the equations listed in papers, that will help facilitate communication and verification of model implementations. To help researchers get started, an example implementation for the full data preparation and batched padded matrix multiplies in JAX is available on GitHub, archived on Zenodo.","title":"Concluding Words"},{"location":"machine-learning/message-passing/#acknowledgments","text":"I thank Rif. A. Saurous for our discussion at the PyMC4 developer summit in Montreal, QC, where his laser-like focus on \u201ctensorify everything\u201d inspired many new thoughts in my mind. Many thanks to my wife, Nan Li, who first pointed me to the linear algebra equivalents of graphs. I also thank David Duvenaud and Matthew J. Johnson for their pedagogy while they were at Harvard.","title":"Acknowledgments"},{"location":"machine-learning/message-passing/#appendix","text":"","title":"Appendix"},{"location":"machine-learning/message-passing/#equivalence-between-padded-and-non-padded-message-passing","text":"To readers who may need an example to be convinced that matrix multiplying the padded matrices is equivalent to matrix multiplying the originals, we show the Python example below. Firstly, without padding: F = np . array ([[ 1 , 0 ], [ 1 , 1 ]]) A = np . array ([[ 1 , 0 ], [ 0 , 1 ]]) M = np . dot ( A , F ) # Value of M # DeviceArray([[1, 0], # [1, 1]], dtype=int32) And now, with padding: pad_size = 2 F_pad = np . pad ( F , pad_width = [ ( 0 , pad_size ), ( 0 , 0 ), ] ) A_pad = np . pad ( A , pad_width = [ ( 0 , pad_size ), ( 0 , pad_size ), ] ) # F_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32) # A_pad: # DeviceArray([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 0], # [0, 0, 0, 0]], dtype=int32) M_pad = np . dot ( A_pad , F_pad ) # M_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32)","title":"Equivalence between padded and non-padded message passing"},{"location":"machine-learning/reimplementing-models/","text":"Reimplementing and Testing Deep Learning Models At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting. As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX). Benefits of re-implementing deep learning models Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested. Developing familiarity with deep learning frameworks Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.) My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages. Developing a mechanistic understanding of the model It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction. Developing an ability to test and sanity-check the model If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model. Reimplementing models is not a waste of time By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste. A proposed workflow for reimplementing deep learning models I will now propose a workflow for re-implementing deep learning models. Identify a coding partner Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model. Doing so helps a few ways. Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up. Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing. Pick out the \"forward\" step of the model The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations. A few keywords to look out for are the forward() and __call__() class methods. class MyModel ( nn . Model ): # ... def forward ( self , X ): # Implementation of model happens here. something = ... return something For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called encoder() and decoder() , which themselves reference another model that would have a forward() or __call__() defined. class AutoEncoder ( nn . Model ): # ... def forward ( self , X ): something = self . encoder ( X ) output = self . decoder ( something ) return output Re-implementing the forward() part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data. Inspect the shapes of the weights While the equations give the model structure , the weights and biases, or the parameters , are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside the priors, and fit the model parameters.) Because much of deep learning hinges on linear algebra, and because most of the transformations that happen involve transforming the input space into the output space , getting the shapes of the parameters is very important. In a re-implementation exercise with my intern, where we re-implemented a specially designed recurrent neural network layer in JAX, we did a manual sanity check through our implementation to identify what the shapes would need to be for the inputs and outputs. After that, we encoded that manual test into an automatic test. Later on, after we built another test that integrated which paradoxically failed on shapes, we eventually uncovered that we were indexing into the wrong dimensions in our implementation. This led to us (1) fixing the bug, (2) writing a more comprehensive documentation suite, and (3) writing better documentations for the semantic meaning of each tensor axis. Write tests for the neural network components Once we have the neural network model and its components implemented, writing tests for those components is a wonderful way of making sure that (1) the implementation is correct, to the best of our knowledge, and that (2) we can catch when the implementation might have been broken inadvertently. The shape test (as described above) is one way of doing this. def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data ) assert output . shape [ 1 ] == output_dims If there are special elementwise transforms performed on the data, such as a ReLU or exponential transform, we can test that the numerical properties of the output are correct: def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data , nonlinearity = \"relu\" ) assert np . all ( output >= 0 ) Write tests for the entire training loop Once the model has been re-implemented in its entirety, prepare a small set of training data, and pass it through the model, and attempt to train it for a few epochs. If the model, as implemented, is doing what we think it should be, then after a dozen epochs or so, the training loss should go down. We can then test that the training loss at the end is less than the training loss at the beginning. If the loss does go down, it's necessary but not sufficient for knowing that the model is implemented correctly. However, if the loss does not go down, then we will definitely know that a problem exists somewhere in the code, and can begin to debug. An example with pseudocode below might look like the following: from data import dummy_graph_data from model import gnn_model from params import make_gnn_params from losses import mse_loss from jax import grad from jax.experimental.optimizers import adam def test_gnn_training (): # Prepare training data x , y = dummy_graph_data ( * args , ** kwargs ) params = make_gnn_params ( * args , ** kwargs ) dloss = grad ( mse_loss ) init , update , get_params = adam ( step_size = 0.005 ) start_loss = mse_loss ( params , model , x , y ) state = init ( params ) for i in range ( 10 ): g = dloss ( params , model , x , y ) state = update ( i , g , state ) params = get_params ( state ) end_loss = mse_loss ( params , model , x , y ) assert end_loss < start_loss A side benefit of this is that if you commit to only judiciously changing the tests, you will end up with a stable and copy/paste-able training loop that you know you can trust on new learning tasks, and hence only need to worry about swapping out the data. Build little tools for yourself that automate repetitive (boring) things You may notice in the above integration test, we wrote a lot of other functions that make testing much easier, such as dummy data generators, and parameter initializers. These are tools that make composing parts of the entire training process modular and easy to compose. I strongly recommend writing these things, and also backing them with more tests (since we will end up relying on them anyways). Now run your deep learning experiments Once we have the model re-implemented and tested, the groundwork is present for us to conduct extensive experiments with the confidence that we know how to catch bugs in the model in a fairly automated fashion. Concluding words Re-implementing deep learning models can be a very fun and rewarding exercise, because it serves as an excellent tool to check our understanding of the models that we work with. Without the right safeguards in place, though, it can also very quickly metamorphose into a nightmare rabbithole of debugging. Placing basic safeguards in place when re-implementing models helps us avoid as many of these rabbitholes as possible.","title":"Reimplementing and Testing Deep Learning Models"},{"location":"machine-learning/reimplementing-models/#reimplementing-and-testing-deep-learning-models","text":"At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting. As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX).","title":"Reimplementing and Testing Deep Learning Models"},{"location":"machine-learning/reimplementing-models/#benefits-of-re-implementing-deep-learning-models","text":"Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested.","title":"Benefits of re-implementing deep learning models"},{"location":"machine-learning/reimplementing-models/#developing-familiarity-with-deep-learning-frameworks","text":"Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.) My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages.","title":"Developing familiarity with deep learning frameworks"},{"location":"machine-learning/reimplementing-models/#developing-a-mechanistic-understanding-of-the-model","text":"It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction.","title":"Developing a mechanistic understanding of the model"},{"location":"machine-learning/reimplementing-models/#developing-an-ability-to-test-and-sanity-check-the-model","text":"If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model.","title":"Developing an ability to test and sanity-check the model"},{"location":"machine-learning/reimplementing-models/#reimplementing-models-is-not-a-waste-of-time","text":"By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste.","title":"Reimplementing models is not a waste of time"},{"location":"machine-learning/reimplementing-models/#a-proposed-workflow-for-reimplementing-deep-learning-models","text":"I will now propose a workflow for re-implementing deep learning models.","title":"A proposed workflow for reimplementing deep learning models"},{"location":"machine-learning/reimplementing-models/#identify-a-coding-partner","text":"Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model. Doing so helps a few ways. Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up. Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing.","title":"Identify a coding partner"},{"location":"machine-learning/reimplementing-models/#pick-out-the-forward-step-of-the-model","text":"The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations. A few keywords to look out for are the forward() and __call__() class methods. class MyModel ( nn . Model ): # ... def forward ( self , X ): # Implementation of model happens here. something = ... return something For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called encoder() and decoder() , which themselves reference another model that would have a forward() or __call__() defined. class AutoEncoder ( nn . Model ): # ... def forward ( self , X ): something = self . encoder ( X ) output = self . decoder ( something ) return output Re-implementing the forward() part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data.","title":"Pick out the \"forward\" step of the model"},{"location":"machine-learning/reimplementing-models/#inspect-the-shapes-of-the-weights","text":"While the equations give the model structure , the weights and biases, or the parameters , are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside the priors, and fit the model parameters.) Because much of deep learning hinges on linear algebra, and because most of the transformations that happen involve transforming the input space into the output space , getting the shapes of the parameters is very important. In a re-implementation exercise with my intern, where we re-implemented a specially designed recurrent neural network layer in JAX, we did a manual sanity check through our implementation to identify what the shapes would need to be for the inputs and outputs. After that, we encoded that manual test into an automatic test. Later on, after we built another test that integrated which paradoxically failed on shapes, we eventually uncovered that we were indexing into the wrong dimensions in our implementation. This led to us (1) fixing the bug, (2) writing a more comprehensive documentation suite, and (3) writing better documentations for the semantic meaning of each tensor axis.","title":"Inspect the shapes of the weights"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-neural-network-components","text":"Once we have the neural network model and its components implemented, writing tests for those components is a wonderful way of making sure that (1) the implementation is correct, to the best of our knowledge, and that (2) we can catch when the implementation might have been broken inadvertently. The shape test (as described above) is one way of doing this. def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data ) assert output . shape [ 1 ] == output_dims If there are special elementwise transforms performed on the data, such as a ReLU or exponential transform, we can test that the numerical properties of the output are correct: def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data , nonlinearity = \"relu\" ) assert np . all ( output >= 0 )","title":"Write tests for the neural network components"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-entire-training-loop","text":"Once the model has been re-implemented in its entirety, prepare a small set of training data, and pass it through the model, and attempt to train it for a few epochs. If the model, as implemented, is doing what we think it should be, then after a dozen epochs or so, the training loss should go down. We can then test that the training loss at the end is less than the training loss at the beginning. If the loss does go down, it's necessary but not sufficient for knowing that the model is implemented correctly. However, if the loss does not go down, then we will definitely know that a problem exists somewhere in the code, and can begin to debug. An example with pseudocode below might look like the following: from data import dummy_graph_data from model import gnn_model from params import make_gnn_params from losses import mse_loss from jax import grad from jax.experimental.optimizers import adam def test_gnn_training (): # Prepare training data x , y = dummy_graph_data ( * args , ** kwargs ) params = make_gnn_params ( * args , ** kwargs ) dloss = grad ( mse_loss ) init , update , get_params = adam ( step_size = 0.005 ) start_loss = mse_loss ( params , model , x , y ) state = init ( params ) for i in range ( 10 ): g = dloss ( params , model , x , y ) state = update ( i , g , state ) params = get_params ( state ) end_loss = mse_loss ( params , model , x , y ) assert end_loss < start_loss A side benefit of this is that if you commit to only judiciously changing the tests, you will end up with a stable and copy/paste-able training loop that you know you can trust on new learning tasks, and hence only need to worry about swapping out the data.","title":"Write tests for the entire training loop"},{"location":"machine-learning/reimplementing-models/#build-little-tools-for-yourself-that-automate-repetitive-boring-things","text":"You may notice in the above integration test, we wrote a lot of other functions that make testing much easier, such as dummy data generators, and parameter initializers. These are tools that make composing parts of the entire training process modular and easy to compose. I strongly recommend writing these things, and also backing them with more tests (since we will end up relying on them anyways).","title":"Build little tools for yourself that automate repetitive (boring) things"},{"location":"machine-learning/reimplementing-models/#now-run-your-deep-learning-experiments","text":"Once we have the model re-implemented and tested, the groundwork is present for us to conduct extensive experiments with the confidence that we know how to catch bugs in the model in a fairly automated fashion.","title":"Now run your deep learning experiments"},{"location":"machine-learning/reimplementing-models/#concluding-words","text":"Re-implementing deep learning models can be a very fun and rewarding exercise, because it serves as an excellent tool to check our understanding of the models that we work with. Without the right safeguards in place, though, it can also very quickly metamorphose into a nightmare rabbithole of debugging. Placing basic safeguards in place when re-implementing models helps us avoid as many of these rabbitholes as possible.","title":"Concluding words"},{"location":"software-skills/","text":"Software Skills Because our day-to-day involves writing code, I am convinced that we data scientists need to be equipped with basic software engineering skills. Being equipped with these skills will help us write code that is, in the long-run, easy to recap, remember, reference, review, and rewrite. In this collection of short essays, I will highlight the basic software skills that, if we master, will increase our efficiency and effectiveness in the long-run. Common Objections If you have heard these suggestions before, then you might have also heard some of the common objections to learning these software practices. I wish to address them here in bulk, so I do not have to address them in-depth in the individual essays. I have not enough time This objection is one I am sympathetic to, as I operate under time constraints myself. This is the nature of code: written once, used many times. Hence, the best response that I can give is that time taken cutting corners now yields multiples of others' (including your future self's) time wasted navigating an undocumented, spaghetti-code codebase, that is not well-structured either. Cutting out these software practices now makes things much more difficult to maintain and improve code when it goes into production. My code is only going to be written and read by myself At some point, though, there is a high probability that you will end up writing code that someone else has to read and use. The time invested in making the code read well now , even on code that does not have to be read by others, will reduce the learning curve pain when you eventually do have to write code for others. You might as well invest the time now while there's less formal scrutiny to practice your software skills. When the stakes are higher, being ready can only be helpful. I don't know how to get started, there are so many places to begin Pick any one skill, say, refactoring, and work on it first. You can always add on more skills into your toolkit as you go along.","title":"Software Skills"},{"location":"software-skills/#software-skills","text":"Because our day-to-day involves writing code, I am convinced that we data scientists need to be equipped with basic software engineering skills. Being equipped with these skills will help us write code that is, in the long-run, easy to recap, remember, reference, review, and rewrite. In this collection of short essays, I will highlight the basic software skills that, if we master, will increase our efficiency and effectiveness in the long-run.","title":"Software Skills"},{"location":"software-skills/#common-objections","text":"If you have heard these suggestions before, then you might have also heard some of the common objections to learning these software practices. I wish to address them here in bulk, so I do not have to address them in-depth in the individual essays.","title":"Common Objections"},{"location":"software-skills/#i-have-not-enough-time","text":"This objection is one I am sympathetic to, as I operate under time constraints myself. This is the nature of code: written once, used many times. Hence, the best response that I can give is that time taken cutting corners now yields multiples of others' (including your future self's) time wasted navigating an undocumented, spaghetti-code codebase, that is not well-structured either. Cutting out these software practices now makes things much more difficult to maintain and improve code when it goes into production.","title":"I have not enough time"},{"location":"software-skills/#my-code-is-only-going-to-be-written-and-read-by-myself","text":"At some point, though, there is a high probability that you will end up writing code that someone else has to read and use. The time invested in making the code read well now , even on code that does not have to be read by others, will reduce the learning curve pain when you eventually do have to write code for others. You might as well invest the time now while there's less formal scrutiny to practice your software skills. When the stakes are higher, being ready can only be helpful.","title":"My code is only going to be written and read by myself"},{"location":"software-skills/#i-dont-know-how-to-get-started-there-are-so-many-places-to-begin","text":"Pick any one skill, say, refactoring, and work on it first. You can always add on more skills into your toolkit as you go along.","title":"I don't know how to get started, there are so many places to begin"},{"location":"software-skills/code-formatting/","text":"Formatting your code One key insight from the Python programming language is that code is read more often than it is written. Hence, writing code in a fashion that makes it easy to read is something that can only be beneficial. But formatting code is a nit-picky and tedious matter, isn't it? Moreover, code style is one of those things that are not substantive enough to engage in flame wars. It really is one of those things we should just get over with, right? Yes, and it is possible to be \"just over and done with it\" if we use automation tools to help us take care of code formatting so that we don't have to think about it. Introducing black black is an opinionated code formatter for the Python programming language. It comes with sane defaults, and produces consistently formatted code with a single command at the terminal. Installing black To install it, we can either use pip or conda : # for pip users pip install black # for conda users conda install black Using black We can use black directly at the command line in our project directory, with configurations called at the command line for convenience. # Format all .py files within and underneath current working directory. black -l 79 . Introducing isort isort is a package for sorting your imports in a source .py file. Once again, this is the sort of thing you definitely don't want to do by hand. Installing isort isort is also conda- and pip-installable. # pip users pip install isort # conda users conda install isort Using isort Just like with black, we can use isort to automagically sort our imports. As an example we will call it at the command line with certain options enabled. # -r: recurses down below the current working directory. # -y: automatically overwrite original source file with sorted imports. isort -r -y . Building automation for code formatting Automatically executing automagic commands is pretty awesome. Let's see how we can enable this. Makefiles I also place black as part of a series of commands used in code style checking in a Makefile, to run all of those commands together. format : isort -r -y . black -l 79 . With that Makefile command, we can now execute all code formatting commands with a single call. Side note: I usually do isort first because black will make detect isort -ed code as not properly formatted, hence I defer to black to make the final changes. Pre-commit hooks We can also use pre-commit hooks to catch non-properly-formatted code, and run the code formatters over the code, preventing them from being merged if any formatting has to take place. This ensures thatwe never commit code that is incorrectly formatted. Getting set up with pre-commit hooks is another topic, but there are already great resources that can be searched for online on how to get setup. Concluding words I hope this short essay gives you an overview of the tools that you can use to format your code automatically. Code formatting is important for readability, but isn't worth the tedium. Letting automation save your time is the wise thing to do.","title":"Formatting your code"},{"location":"software-skills/code-formatting/#formatting-your-code","text":"One key insight from the Python programming language is that code is read more often than it is written. Hence, writing code in a fashion that makes it easy to read is something that can only be beneficial. But formatting code is a nit-picky and tedious matter, isn't it? Moreover, code style is one of those things that are not substantive enough to engage in flame wars. It really is one of those things we should just get over with, right? Yes, and it is possible to be \"just over and done with it\" if we use automation tools to help us take care of code formatting so that we don't have to think about it.","title":"Formatting your code"},{"location":"software-skills/code-formatting/#introducing-black","text":"black is an opinionated code formatter for the Python programming language. It comes with sane defaults, and produces consistently formatted code with a single command at the terminal.","title":"Introducing black"},{"location":"software-skills/code-formatting/#installing-black","text":"To install it, we can either use pip or conda : # for pip users pip install black # for conda users conda install black","title":"Installing black"},{"location":"software-skills/code-formatting/#using-black","text":"We can use black directly at the command line in our project directory, with configurations called at the command line for convenience. # Format all .py files within and underneath current working directory. black -l 79 .","title":"Using black"},{"location":"software-skills/code-formatting/#introducing-isort","text":"isort is a package for sorting your imports in a source .py file. Once again, this is the sort of thing you definitely don't want to do by hand.","title":"Introducing isort"},{"location":"software-skills/code-formatting/#installing-isort","text":"isort is also conda- and pip-installable. # pip users pip install isort # conda users conda install isort","title":"Installing isort"},{"location":"software-skills/code-formatting/#using-isort","text":"Just like with black, we can use isort to automagically sort our imports. As an example we will call it at the command line with certain options enabled. # -r: recurses down below the current working directory. # -y: automatically overwrite original source file with sorted imports. isort -r -y .","title":"Using isort"},{"location":"software-skills/code-formatting/#building-automation-for-code-formatting","text":"Automatically executing automagic commands is pretty awesome. Let's see how we can enable this.","title":"Building automation for code formatting"},{"location":"software-skills/code-formatting/#makefiles","text":"I also place black as part of a series of commands used in code style checking in a Makefile, to run all of those commands together. format : isort -r -y . black -l 79 . With that Makefile command, we can now execute all code formatting commands with a single call. Side note: I usually do isort first because black will make detect isort -ed code as not properly formatted, hence I defer to black to make the final changes.","title":"Makefiles"},{"location":"software-skills/code-formatting/#pre-commit-hooks","text":"We can also use pre-commit hooks to catch non-properly-formatted code, and run the code formatters over the code, preventing them from being merged if any formatting has to take place. This ensures thatwe never commit code that is incorrectly formatted. Getting set up with pre-commit hooks is another topic, but there are already great resources that can be searched for online on how to get setup.","title":"Pre-commit hooks"},{"location":"software-skills/code-formatting/#concluding-words","text":"I hope this short essay gives you an overview of the tools that you can use to format your code automatically. Code formatting is important for readability, but isn't worth the tedium. Letting automation save your time is the wise thing to do.","title":"Concluding words"},{"location":"software-skills/documentation/","text":"Documenting your code Writing lightweight documentation is a practice that I found sorely lacking in data science practice. In this essay, I will show you how to introduce lightweight documentation into your code. Why document your code There are a few good reasons to document your code. Firstly, your future self will thank you for having a plain English translation of what you intended to do with that block of code. Oftentimes, the intent behind the code is lost in the translation from our heads to actual code. Secondly, other readers of your code will also thank you. Thirdly, by clarifying what exactly you intended to accomplish with a block of code, as well as the major steps taken towards accomplishing those goals, you often will end up with a much cleaner implementation in the end. When to document your code A pragmatic choice would be once you find yourself accomplishing a logical chunk of work. I usually do it as soon as I define a Python function. Where your code documentation should go As a general rule of thumb, having code documentation as close to the actual source code is probably the best way to approach this. For Python programmers, this would imply taking advantage of docstrings ! Docstrings occur in the following places: Right after a function or class method definition. Right inside a class definition. Right at the top of a .py module. An anti-pattern here would be writing your documentation in an external system, such as a Wiki. (Woe betide the code developer who writes code docs in Confluence...) This is because the documentation is not proximal to the source code. I have found myself forgetting to update the docstrings after updating the source code. If it's easy to forget to update the docs when the docs are right next to the source, imagine how much easier it is to forget to update external docs! Where, then, would documentation on how the code is organized live then? I would argue it should be pushed as close to the source code as possible. For example, we can use the .py module docstrings to describe the intent behind why certain entire modules exist. An example Here is a skeleton to follow: \"\"\" This module houses all functions that cannot be neatly categorized in other places. \"\"\" def my_function ( arg1 , arg2 ): \"\"\" Calculates something based on arg1 and arg2. This calculated thing is intended to be used by `this_other_function`, so the return type should not be changed. :param arg1: Describe arg1 :param arg2: Describe arg2 :returns: ``the_thing_being_returned``, a pandas DataFrame (for example). \"\"\" the_thing_being_returned = ... # implement the function return the_thing_being_returned Now, let's see this in action with a function that returns a snake-cased version of a string with all punctuation also removed. (This is a simplified implementation of what is implemented in pyjanitor 's clean_names function.) import string def clean_string ( s ): \"\"\" Remove all punctuation from string, and convert to lower_snake_case. An example of the input and output: \"My string!\" -> \"my_string\" :param s: String to clean. \"\"\" s = s . replace ( string . punctuation , \"_\" ) . replace ( \" \" , \"_\" ) . strip ( \"_\" ) . lower () return s You may notice that the docstring is longer than the implementation. Frequently (though not always), I have found that when docstring length exceeds implementation length, it is a sign that the author(s) of the code have been thoughtful about its implementation. This bodes well for working in a team, especially when a data scientist hands over a prototype to the engineering team. Addressing objections The main objections to injecting \"basic software engineering\" into a data scientist's workflow usually center around not having enough time. As always, I am sympathetic to this objection, because I also operate under time constraints. One thing I will offer is that docs are an investment of time for the team, rather than for the individual. We save multiples of time downstream when we write good docs. One way to conceptualize this is the number of person-hours saved down the road by oneself and one's teammates when good docs exist. We minimize the amount of time spent reading code to grok what it is about. At the same time, the practice of clarifying what we intend to accomplish with the function can help bring clarity to the implementation. This I have mentioned above. Having a clean implementation makes things easier to maintain later on. Hence, time invested now on good docs also helps us later on. As with other software engineering skills, this is a skill that can be picked up, refined, and honed. We get more efficient at writing docs the more we do it. Parting words I hope this essay has helped you get a feel for how you can write well-documented code. At the same time, I hope that by showing you a simple anchoring example that you will be able to replicate the pattern in your own work.","title":"Documenting your code"},{"location":"software-skills/documentation/#documenting-your-code","text":"Writing lightweight documentation is a practice that I found sorely lacking in data science practice. In this essay, I will show you how to introduce lightweight documentation into your code.","title":"Documenting your code"},{"location":"software-skills/documentation/#why-document-your-code","text":"There are a few good reasons to document your code. Firstly, your future self will thank you for having a plain English translation of what you intended to do with that block of code. Oftentimes, the intent behind the code is lost in the translation from our heads to actual code. Secondly, other readers of your code will also thank you. Thirdly, by clarifying what exactly you intended to accomplish with a block of code, as well as the major steps taken towards accomplishing those goals, you often will end up with a much cleaner implementation in the end.","title":"Why document your code"},{"location":"software-skills/documentation/#when-to-document-your-code","text":"A pragmatic choice would be once you find yourself accomplishing a logical chunk of work. I usually do it as soon as I define a Python function.","title":"When to document your code"},{"location":"software-skills/documentation/#where-your-code-documentation-should-go","text":"As a general rule of thumb, having code documentation as close to the actual source code is probably the best way to approach this. For Python programmers, this would imply taking advantage of docstrings ! Docstrings occur in the following places: Right after a function or class method definition. Right inside a class definition. Right at the top of a .py module. An anti-pattern here would be writing your documentation in an external system, such as a Wiki. (Woe betide the code developer who writes code docs in Confluence...) This is because the documentation is not proximal to the source code. I have found myself forgetting to update the docstrings after updating the source code. If it's easy to forget to update the docs when the docs are right next to the source, imagine how much easier it is to forget to update external docs! Where, then, would documentation on how the code is organized live then? I would argue it should be pushed as close to the source code as possible. For example, we can use the .py module docstrings to describe the intent behind why certain entire modules exist.","title":"Where your code documentation should go"},{"location":"software-skills/documentation/#an-example","text":"Here is a skeleton to follow: \"\"\" This module houses all functions that cannot be neatly categorized in other places. \"\"\" def my_function ( arg1 , arg2 ): \"\"\" Calculates something based on arg1 and arg2. This calculated thing is intended to be used by `this_other_function`, so the return type should not be changed. :param arg1: Describe arg1 :param arg2: Describe arg2 :returns: ``the_thing_being_returned``, a pandas DataFrame (for example). \"\"\" the_thing_being_returned = ... # implement the function return the_thing_being_returned Now, let's see this in action with a function that returns a snake-cased version of a string with all punctuation also removed. (This is a simplified implementation of what is implemented in pyjanitor 's clean_names function.) import string def clean_string ( s ): \"\"\" Remove all punctuation from string, and convert to lower_snake_case. An example of the input and output: \"My string!\" -> \"my_string\" :param s: String to clean. \"\"\" s = s . replace ( string . punctuation , \"_\" ) . replace ( \" \" , \"_\" ) . strip ( \"_\" ) . lower () return s You may notice that the docstring is longer than the implementation. Frequently (though not always), I have found that when docstring length exceeds implementation length, it is a sign that the author(s) of the code have been thoughtful about its implementation. This bodes well for working in a team, especially when a data scientist hands over a prototype to the engineering team.","title":"An example"},{"location":"software-skills/documentation/#addressing-objections","text":"The main objections to injecting \"basic software engineering\" into a data scientist's workflow usually center around not having enough time. As always, I am sympathetic to this objection, because I also operate under time constraints. One thing I will offer is that docs are an investment of time for the team, rather than for the individual. We save multiples of time downstream when we write good docs. One way to conceptualize this is the number of person-hours saved down the road by oneself and one's teammates when good docs exist. We minimize the amount of time spent reading code to grok what it is about. At the same time, the practice of clarifying what we intend to accomplish with the function can help bring clarity to the implementation. This I have mentioned above. Having a clean implementation makes things easier to maintain later on. Hence, time invested now on good docs also helps us later on. As with other software engineering skills, this is a skill that can be picked up, refined, and honed. We get more efficient at writing docs the more we do it.","title":"Addressing objections"},{"location":"software-skills/documentation/#parting-words","text":"I hope this essay has helped you get a feel for how you can write well-documented code. At the same time, I hope that by showing you a simple anchoring example that you will be able to replicate the pattern in your own work.","title":"Parting words"},{"location":"software-skills/refactoring/","text":"Refactoring your code How many times have you found yourself copy/pasting code from one notebook to another? If it the answer is \"many\", then this essay probably has something for you. We're going to look at the practice of \"refactoring\" code, and how it applies in a data science context. Why refactor When writing code, we intend to have a block of code do one thing. As such, its multiple application should have a single source of truth. However, the practice of copying and pasting code gives us multiple sources of truth. Refactoring code, thus, gives us a way of establishing a single source of truth for our functions, which can be called on in multiple situations. When to refactor The short answer is \"basically whenever you find yourself hitting copy+paste\" on your keyboard. How do we refactor The steps involved are as follows. Wrap the semi-complex block of code in a function. Identify what you would consider to be an \"input\" and \"output\" for the function. Take specific variable names and give them more general names. An example Let's take the example of a chunk of code that takes a protein sequence, compares it to a reference sequence, and returns all of the mutations that it has. (We will only implemenet a naive version for the sake of pedagogy.) sequence1 = ... sequence2 = ... mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( sequence1 , sequence2 )): mutations . append ( f \"{letter1}{i+1}{letter2}\" ) mutations = \"; \" . join ( m for m in mutations ) This more or less should accomplish what we want. Let's now apply the ideas behind refactoring to this code block. def mutation_string ( reference , sequence , sep = \"; \" ): mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( reference , sequence )): mutations . append ( f \"{letter1}{i+1}{letter2}\" ) return f \"{sep}\" . join ( m for m in mutations ) You'll notice the three steps coming into play. Firstly , we simply shifted the main logic of the code into a function definition. Secondly , we then generalized the function a bit, by renaming sequence1 and sequence2 to what we usually intend for it to be, a sequence of interest and a reference sequence. Finally , we defined those two as inputs, alongside a keyword argument called sep , which defines the separator between each mutation. Bonus On the basis of this function definition, we can do some additional neat things! For example, in protein sequence analysis, our reference sequence is usually kept constant. Hence, we can actually create a custom mutation_string for our reference sequence using functools.partial by fixing reference to a particular value, thus eliminating the need to repetitively pass in the same reference string. from functools import partial protein1 = ... # define the string here. prot1_mut_string = partial ( mutation_string , reference = protein1 ) protein2 = ... # define the string here. mutstring = prot1_mut_string ( sequence = protein2 ) Where should this function be refactored to You can choose to keep it in the notebook, and that would be fine if the function was used only in a single notebook. If you find yourself needing to call on that same function from another notebook, do the right thing and create a utils.py (or analogous) Python module that lives in the same directory as the notebook. Then, import the refactored function from utils.py . If you feel sophisticated, you can also create a custom Python library for your project. I will address this in a separate essay. An anti-pattern, though, would be to attempt to treat the notebook as source code and import the function from one notebook into another. Notebooks are great for one thing: weaving functions together into an integrarted analysis. I'm of the opinion that we should use a tool the way it was intended, and bring in other tools to do what we need. In this respect, I think that DataBricks notebooks does the wrong thing by bowing to bad human first instincts rather than encouraging productive behaviours. Where do we find time to do this I hear this concern, as I went through the same concerns myself. Isn't it faster to just copy/paste the code? What if I don't end up reusing the code elsewhere? Isn't the time then wasted? In thinking back to my own habits, I realized early on that doing this was not a matter of technical ability but rather a matter of mindset. Investing the time into doing simple refactoring alongside my analyses does take immediate time away from the analysis. However, the deliberate practice of refactoring early on earns back multiples of the time spent as the project progresses. Moreover, if and when the project gets handed over \"in production\", or at least shared with others to use, our colleagues can spend less time is spent navigating a spaghetti-like codebase, and more time can be spent building a proper mental model of the codebase to build on top of. On the possiblity of not reusing the code elsewhere, I would strongly disagree. Refactoring is not a common skill, while copy/pasting code is. Every chance we get to refactor code is practicing the skill, which only gets sharper and more refined as we do it more. Hence, even for the sake of getting more practice makes it worthwhile to do refactoring at every chance. Concluding words I hope this mini-essay demystifies the practice of code refactoring, and gives you some ideas on how to make it part of your workflow.","title":"Refactoring your code"},{"location":"software-skills/refactoring/#refactoring-your-code","text":"How many times have you found yourself copy/pasting code from one notebook to another? If it the answer is \"many\", then this essay probably has something for you. We're going to look at the practice of \"refactoring\" code, and how it applies in a data science context.","title":"Refactoring your code"},{"location":"software-skills/refactoring/#why-refactor","text":"When writing code, we intend to have a block of code do one thing. As such, its multiple application should have a single source of truth. However, the practice of copying and pasting code gives us multiple sources of truth. Refactoring code, thus, gives us a way of establishing a single source of truth for our functions, which can be called on in multiple situations.","title":"Why refactor"},{"location":"software-skills/refactoring/#when-to-refactor","text":"The short answer is \"basically whenever you find yourself hitting copy+paste\" on your keyboard.","title":"When to refactor"},{"location":"software-skills/refactoring/#how-do-we-refactor","text":"The steps involved are as follows. Wrap the semi-complex block of code in a function. Identify what you would consider to be an \"input\" and \"output\" for the function. Take specific variable names and give them more general names.","title":"How do we refactor"},{"location":"software-skills/refactoring/#an-example","text":"Let's take the example of a chunk of code that takes a protein sequence, compares it to a reference sequence, and returns all of the mutations that it has. (We will only implemenet a naive version for the sake of pedagogy.) sequence1 = ... sequence2 = ... mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( sequence1 , sequence2 )): mutations . append ( f \"{letter1}{i+1}{letter2}\" ) mutations = \"; \" . join ( m for m in mutations ) This more or less should accomplish what we want. Let's now apply the ideas behind refactoring to this code block. def mutation_string ( reference , sequence , sep = \"; \" ): mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( reference , sequence )): mutations . append ( f \"{letter1}{i+1}{letter2}\" ) return f \"{sep}\" . join ( m for m in mutations ) You'll notice the three steps coming into play. Firstly , we simply shifted the main logic of the code into a function definition. Secondly , we then generalized the function a bit, by renaming sequence1 and sequence2 to what we usually intend for it to be, a sequence of interest and a reference sequence. Finally , we defined those two as inputs, alongside a keyword argument called sep , which defines the separator between each mutation.","title":"An example"},{"location":"software-skills/refactoring/#bonus","text":"On the basis of this function definition, we can do some additional neat things! For example, in protein sequence analysis, our reference sequence is usually kept constant. Hence, we can actually create a custom mutation_string for our reference sequence using functools.partial by fixing reference to a particular value, thus eliminating the need to repetitively pass in the same reference string. from functools import partial protein1 = ... # define the string here. prot1_mut_string = partial ( mutation_string , reference = protein1 ) protein2 = ... # define the string here. mutstring = prot1_mut_string ( sequence = protein2 )","title":"Bonus"},{"location":"software-skills/refactoring/#where-should-this-function-be-refactored-to","text":"You can choose to keep it in the notebook, and that would be fine if the function was used only in a single notebook. If you find yourself needing to call on that same function from another notebook, do the right thing and create a utils.py (or analogous) Python module that lives in the same directory as the notebook. Then, import the refactored function from utils.py . If you feel sophisticated, you can also create a custom Python library for your project. I will address this in a separate essay. An anti-pattern, though, would be to attempt to treat the notebook as source code and import the function from one notebook into another. Notebooks are great for one thing: weaving functions together into an integrarted analysis. I'm of the opinion that we should use a tool the way it was intended, and bring in other tools to do what we need. In this respect, I think that DataBricks notebooks does the wrong thing by bowing to bad human first instincts rather than encouraging productive behaviours.","title":"Where should this function be refactored to"},{"location":"software-skills/refactoring/#where-do-we-find-time-to-do-this","text":"I hear this concern, as I went through the same concerns myself. Isn't it faster to just copy/paste the code? What if I don't end up reusing the code elsewhere? Isn't the time then wasted? In thinking back to my own habits, I realized early on that doing this was not a matter of technical ability but rather a matter of mindset. Investing the time into doing simple refactoring alongside my analyses does take immediate time away from the analysis. However, the deliberate practice of refactoring early on earns back multiples of the time spent as the project progresses. Moreover, if and when the project gets handed over \"in production\", or at least shared with others to use, our colleagues can spend less time is spent navigating a spaghetti-like codebase, and more time can be spent building a proper mental model of the codebase to build on top of. On the possiblity of not reusing the code elsewhere, I would strongly disagree. Refactoring is not a common skill, while copy/pasting code is. Every chance we get to refactor code is practicing the skill, which only gets sharper and more refined as we do it more. Hence, even for the sake of getting more practice makes it worthwhile to do refactoring at every chance.","title":"Where do we find time to do this"},{"location":"software-skills/refactoring/#concluding-words","text":"I hope this mini-essay demystifies the practice of code refactoring, and gives you some ideas on how to make it part of your workflow.","title":"Concluding words"},{"location":"software-skills/testing/","text":"Testing your code Writing tests for code is a basic software skill. Writing tests helps build confidence in the stability of our code. When to write tests There are two \"time scales\" at which I think this question can be answered. The first time scale is \"short-term\". As soon as we finish up a function, that first test should be written. Doing so lets us immediately sanity-check our intuition about the newly-written fuction. The second time scale is \"longer-term\". As soon as we discover bugs, new tests should be added to the test suite. Those new tests should either cover that exact bug, or cover the class of bugs together. How to get setup In a Python project, first ensure that you have pytest installed. If you follow recommended practice and have one conda environment per project, then you should be able to install pytest using conda : # if you use conda: conda install pytest # if you use pip: pip install pytest The anatomy of a test When using pytest , your tests take on the function name: from custom_library import my_function def test_my_function (): \"\"\"Test for my_function.\"\"\" # set up test here. assert some_condition We can then execute the test from the command line: pytest . Voila! The tests will be executed, and you will see them run one by one. The kinds of tests you could write Let's go through the kinds of tests you might want to write. Execution tests I started with this kind of test because these are the simplest to understand: we simply execute a function to make sure that it runs without breaking. from custom_lib import my_function def test_my_function (): \"\"\"Execution test for my_function.\"\"\" my_function () This kind of test is useful when your function is not parameterized, and simply calls on other functions inside your library. It is also incredibly useful as a starter test when you cannot think of a better test to write. One place where I have used this test pattern is when we built a project dashboard using Panel. The dashboard is made from many complex layers of function calls, involving database queries, data preprocessing, cached results, and more. Sporadically, something would break, and it was something difficult to debug. By wrapping the dashboard execution inside a Python function and executing it by simply calling dashboard() , we could discover bugs as soon as they showed up, rather than so-called \"in production\". Example-based test An example-based test looks basically like this: from custom_lib import another_function def test_another_function (): arg1 = ... arg2 = ... result = another_function ( arg1 , arg2 ) expected_result = ... assert result == expected_result Basically, we set up the test with an example, and check that when given a set of pre-specified inputs, a particular expected result is returned. When writing code in the notebook, I find myself writing example-based tests informally all the time. They are those \"sanity-checks\" function calls where I manually check that the result looks correct. I am sure you do too. So rather than rely on manually checking, it makes perfect sense to simply copy and paste the code into a test function and execute them. Advanced Testing The above I consider to be basic, bare minimum testing that a data scientist can do. Of course, there are more complex forms of testing that a QA engineer would engage in, and I find it useful to know at least what they are and what tools we have to do these forms of testing in the Python ecosystem: Parameterized tests: pytest has these capabilities . Property-based tests: hypothesis gives us these capabilities . Tests for Data Data are notoriously difficult to test, because it is a snapshot of the stochastic state of the world. Nonetheless, if we impose prior knowledge on our testing, we can ensure that certain errors in our data never show up. Nullity Tests For example, if we subject a SQL query to a series of transforms that are supposed to guarantee a densely populated DataFrame, then we can write a nullity test . def test_dataframe_function (): \"\"\"Ensures that there are no null values in the dataframe function.\"\"\" df = dataframe_function ( * args , ** kwargs ) assert pd . isnull ( df ) . sum () . sum () == 0 dtype Tests We can also check that the dtypes of the dataframe are correct. def test_dataframe_dtypes (): \"\"\"Checks that the dtypes of the dataframe are correct.\"\"\" dtypes = { \"col1\" : float32 , \"col2\" : int , \"col3\" : object , } df = dataframe_function ( * args , ** kwargs ) for col , dtype in dtypes . items (): assert df [ col ] . dtype == dtype Bounds Tests We can also check to make sure that our dataframe-returning function yields data in the correct bounds for each column. def test_dataframe_bounds (): \"\"\"Checks that the bounds of datsa are correct.\"\"\" df = dataframe_function ( * args , ** kwargs ) # For a column that can be greater than or equal to zero. assert df [ \"column1\" ] . min () >= 0 # For a column that can only be non-zero positive. assert df [ \"column2\" ] . min () > 0 # For a column that can only be non-zero negative. assert df [ \"column3\" ] . max () < 0 DataFrame tests are a special one for data scientists, because the dataframe is the idiomatic data structure that we engage with on an almost daily basis. Column Name Tests Having stable and consistent column names in the dataframes that we use is extremely important; the column names are like our API to the data. Hence, checking that a suite of expected column names exist in the dataframe can be very useful. def test_dataframe_names (): \"\"\"Checks that dataframe column names are correct.\"\"\" expected_column_names = [ \"col1\" , \"col2\" , \"col3\" ] df = dataframe_function ( * args , ** kwargs ) # Check that each of those column names are present for c in expected_column_names : assert c in df . columns # (Optional) check that _only_ those columns are present. assert set ( df . columns ) == set ( expected_column_names ) Other statistical property tests Testing the mean, median, and mode are difficult, but under some circumstances, such as when we know that the data are drawn from some distribution, we might be able to write a test for the central tendencies of the data. Placing an automated test that checks whether the data matches a particular parameterized distribution with some probability value is generally not a good idea, because it can give a false sense of security . However, if this is a key modelling assumption and you need to keep an automated, rolling check on your data, then having it as a test can help you catch failures in downstream modelling early. In practice, I rarely use this because the speed at which data come in are slow relative to the time I need to check assumptions. Additionally, the stochastic nature of data means that this test would be a flaky one, which is an undesirable property for tests. Parting words I hope this essay gives you some ideas for implementing testing in your data science workflow. As with other software skills, these are skills that become muscle memory over time, hence taking the time from our daily hustle to practice them makes us more efficient in the long-run. In particular, the consistent practice of testing builds confidence in our codebase, not just for my future self, but also for other colleagues who might end up using the codebase too.","title":"Testing your code"},{"location":"software-skills/testing/#testing-your-code","text":"Writing tests for code is a basic software skill. Writing tests helps build confidence in the stability of our code.","title":"Testing your code"},{"location":"software-skills/testing/#when-to-write-tests","text":"There are two \"time scales\" at which I think this question can be answered. The first time scale is \"short-term\". As soon as we finish up a function, that first test should be written. Doing so lets us immediately sanity-check our intuition about the newly-written fuction. The second time scale is \"longer-term\". As soon as we discover bugs, new tests should be added to the test suite. Those new tests should either cover that exact bug, or cover the class of bugs together.","title":"When to write tests"},{"location":"software-skills/testing/#how-to-get-setup","text":"In a Python project, first ensure that you have pytest installed. If you follow recommended practice and have one conda environment per project, then you should be able to install pytest using conda : # if you use conda: conda install pytest # if you use pip: pip install pytest","title":"How to get setup"},{"location":"software-skills/testing/#the-anatomy-of-a-test","text":"When using pytest , your tests take on the function name: from custom_library import my_function def test_my_function (): \"\"\"Test for my_function.\"\"\" # set up test here. assert some_condition We can then execute the test from the command line: pytest . Voila! The tests will be executed, and you will see them run one by one.","title":"The anatomy of a test"},{"location":"software-skills/testing/#the-kinds-of-tests-you-could-write","text":"Let's go through the kinds of tests you might want to write.","title":"The kinds of tests you could write"},{"location":"software-skills/testing/#execution-tests","text":"I started with this kind of test because these are the simplest to understand: we simply execute a function to make sure that it runs without breaking. from custom_lib import my_function def test_my_function (): \"\"\"Execution test for my_function.\"\"\" my_function () This kind of test is useful when your function is not parameterized, and simply calls on other functions inside your library. It is also incredibly useful as a starter test when you cannot think of a better test to write. One place where I have used this test pattern is when we built a project dashboard using Panel. The dashboard is made from many complex layers of function calls, involving database queries, data preprocessing, cached results, and more. Sporadically, something would break, and it was something difficult to debug. By wrapping the dashboard execution inside a Python function and executing it by simply calling dashboard() , we could discover bugs as soon as they showed up, rather than so-called \"in production\".","title":"Execution tests"},{"location":"software-skills/testing/#example-based-test","text":"An example-based test looks basically like this: from custom_lib import another_function def test_another_function (): arg1 = ... arg2 = ... result = another_function ( arg1 , arg2 ) expected_result = ... assert result == expected_result Basically, we set up the test with an example, and check that when given a set of pre-specified inputs, a particular expected result is returned. When writing code in the notebook, I find myself writing example-based tests informally all the time. They are those \"sanity-checks\" function calls where I manually check that the result looks correct. I am sure you do too. So rather than rely on manually checking, it makes perfect sense to simply copy and paste the code into a test function and execute them.","title":"Example-based test"},{"location":"software-skills/testing/#advanced-testing","text":"The above I consider to be basic, bare minimum testing that a data scientist can do. Of course, there are more complex forms of testing that a QA engineer would engage in, and I find it useful to know at least what they are and what tools we have to do these forms of testing in the Python ecosystem: Parameterized tests: pytest has these capabilities . Property-based tests: hypothesis gives us these capabilities .","title":"Advanced Testing"},{"location":"software-skills/testing/#tests-for-data","text":"Data are notoriously difficult to test, because it is a snapshot of the stochastic state of the world. Nonetheless, if we impose prior knowledge on our testing, we can ensure that certain errors in our data never show up.","title":"Tests for Data"},{"location":"software-skills/testing/#nullity-tests","text":"For example, if we subject a SQL query to a series of transforms that are supposed to guarantee a densely populated DataFrame, then we can write a nullity test . def test_dataframe_function (): \"\"\"Ensures that there are no null values in the dataframe function.\"\"\" df = dataframe_function ( * args , ** kwargs ) assert pd . isnull ( df ) . sum () . sum () == 0","title":"Nullity Tests"},{"location":"software-skills/testing/#dtype-tests","text":"We can also check that the dtypes of the dataframe are correct. def test_dataframe_dtypes (): \"\"\"Checks that the dtypes of the dataframe are correct.\"\"\" dtypes = { \"col1\" : float32 , \"col2\" : int , \"col3\" : object , } df = dataframe_function ( * args , ** kwargs ) for col , dtype in dtypes . items (): assert df [ col ] . dtype == dtype","title":"dtype Tests"},{"location":"software-skills/testing/#bounds-tests","text":"We can also check to make sure that our dataframe-returning function yields data in the correct bounds for each column. def test_dataframe_bounds (): \"\"\"Checks that the bounds of datsa are correct.\"\"\" df = dataframe_function ( * args , ** kwargs ) # For a column that can be greater than or equal to zero. assert df [ \"column1\" ] . min () >= 0 # For a column that can only be non-zero positive. assert df [ \"column2\" ] . min () > 0 # For a column that can only be non-zero negative. assert df [ \"column3\" ] . max () < 0 DataFrame tests are a special one for data scientists, because the dataframe is the idiomatic data structure that we engage with on an almost daily basis.","title":"Bounds Tests"},{"location":"software-skills/testing/#column-name-tests","text":"Having stable and consistent column names in the dataframes that we use is extremely important; the column names are like our API to the data. Hence, checking that a suite of expected column names exist in the dataframe can be very useful. def test_dataframe_names (): \"\"\"Checks that dataframe column names are correct.\"\"\" expected_column_names = [ \"col1\" , \"col2\" , \"col3\" ] df = dataframe_function ( * args , ** kwargs ) # Check that each of those column names are present for c in expected_column_names : assert c in df . columns # (Optional) check that _only_ those columns are present. assert set ( df . columns ) == set ( expected_column_names )","title":"Column Name Tests"},{"location":"software-skills/testing/#other-statistical-property-tests","text":"Testing the mean, median, and mode are difficult, but under some circumstances, such as when we know that the data are drawn from some distribution, we might be able to write a test for the central tendencies of the data. Placing an automated test that checks whether the data matches a particular parameterized distribution with some probability value is generally not a good idea, because it can give a false sense of security . However, if this is a key modelling assumption and you need to keep an automated, rolling check on your data, then having it as a test can help you catch failures in downstream modelling early. In practice, I rarely use this because the speed at which data come in are slow relative to the time I need to check assumptions. Additionally, the stochastic nature of data means that this test would be a flaky one, which is an undesirable property for tests.","title":"Other statistical property tests"},{"location":"software-skills/testing/#parting-words","text":"I hope this essay gives you some ideas for implementing testing in your data science workflow. As with other software skills, these are skills that become muscle memory over time, hence taking the time from our daily hustle to practice them makes us more efficient in the long-run. In particular, the consistent practice of testing builds confidence in our codebase, not just for my future self, but also for other colleagues who might end up using the codebase too.","title":"Parting words"},{"location":"terminal/cli-tools/","text":"Tools and Upgrades for your CLI In this short essay, I would like to introduce you to a list of awesome command-line tools that I have found on the internet. Most of the tools listed here do one thing really well: they add visual clarity to the text that we are looking at. This is mostly done by colorizing the terminal with syntax highlighting. Without further ado, let's get started listing them. exa exa is a favourite of mine, because it is an almost drop-in replacement for ls , except with saner defaults. It also comes with a saner set of defaults for the tree command. After installing, you can replace ls and tree with exa by aliasing: alias ls = 'exa --long --git -a --header --group' alias tree = 'exa --tree --level=2 --long -a --header --git' tmux tmux is another daily driver of mine. I use it to keep remote terminal sessions persistent, and use it effectively as a workspace manager between projects. nanorc If you're like me, and are accustomed to the nano text editor rather than vim or emacs , then nanorc , a set of syntax highlighting configurations provided by Anthony Scopatz is an awesome addition to your nano toolkit. (For what it's worth, I wrote this short essay in nano , and nanorc played no small role in making the text readable!) diff-so-fancy diff-so-fancy is a drop-in replacement for diff , and makes it so much easier read diffs between two files. After installation, you can easily replace diff with diff-so-fancy through aliasing: alias diff = \"diff-so-fancy\" bat bat is another one of those instant favourites. I use cat and less often to look through files, but bat takes things to another level. It is basically a mash-up between cat and less , allowing you to scroll through your files in a less -like scrolling fashion, while also providing syntax highlighting for the files you open. At the same time, it'll let you concatenate two files together (just like cat ) and display them to the screen. After installing, you can replace cat with bat by aliasing as well: alias cat = \"bat\" fd fd is another tool that provides saner syntax than the default find . After installing, you can replace find with fd by aliasing: alias find = \"fd\" ripgrep ripgrep is a tool that will let you search directories recursively for a particular pattern. This can help you quickly find text inside a file inside the file tree easily. References Vim From Scratch introduced many of the tools shown here, and I want to make sure that the author gets credit for finding and sharing these awesome tools! James Weis introduced me to tmux while in grad school, and I've been hooked ever since.","title":"Tools and Upgrades for your CLI"},{"location":"terminal/cli-tools/#tools-and-upgrades-for-your-cli","text":"In this short essay, I would like to introduce you to a list of awesome command-line tools that I have found on the internet. Most of the tools listed here do one thing really well: they add visual clarity to the text that we are looking at. This is mostly done by colorizing the terminal with syntax highlighting. Without further ado, let's get started listing them.","title":"Tools and Upgrades for your CLI"},{"location":"terminal/cli-tools/#exa","text":"exa is a favourite of mine, because it is an almost drop-in replacement for ls , except with saner defaults. It also comes with a saner set of defaults for the tree command. After installing, you can replace ls and tree with exa by aliasing: alias ls = 'exa --long --git -a --header --group' alias tree = 'exa --tree --level=2 --long -a --header --git'","title":"exa"},{"location":"terminal/cli-tools/#tmux","text":"tmux is another daily driver of mine. I use it to keep remote terminal sessions persistent, and use it effectively as a workspace manager between projects.","title":"tmux"},{"location":"terminal/cli-tools/#nanorc","text":"If you're like me, and are accustomed to the nano text editor rather than vim or emacs , then nanorc , a set of syntax highlighting configurations provided by Anthony Scopatz is an awesome addition to your nano toolkit. (For what it's worth, I wrote this short essay in nano , and nanorc played no small role in making the text readable!)","title":"nanorc"},{"location":"terminal/cli-tools/#diff-so-fancy","text":"diff-so-fancy is a drop-in replacement for diff , and makes it so much easier read diffs between two files. After installation, you can easily replace diff with diff-so-fancy through aliasing: alias diff = \"diff-so-fancy\"","title":"diff-so-fancy"},{"location":"terminal/cli-tools/#bat","text":"bat is another one of those instant favourites. I use cat and less often to look through files, but bat takes things to another level. It is basically a mash-up between cat and less , allowing you to scroll through your files in a less -like scrolling fashion, while also providing syntax highlighting for the files you open. At the same time, it'll let you concatenate two files together (just like cat ) and display them to the screen. After installing, you can replace cat with bat by aliasing as well: alias cat = \"bat\"","title":"bat"},{"location":"terminal/cli-tools/#fd","text":"fd is another tool that provides saner syntax than the default find . After installing, you can replace find with fd by aliasing: alias find = \"fd\"","title":"fd"},{"location":"terminal/cli-tools/#ripgrep","text":"ripgrep is a tool that will let you search directories recursively for a particular pattern. This can help you quickly find text inside a file inside the file tree easily.","title":"ripgrep"},{"location":"terminal/cli-tools/#references","text":"Vim From Scratch introduced many of the tools shown here, and I want to make sure that the author gets credit for finding and sharing these awesome tools! James Weis introduced me to tmux while in grad school, and I've been hooked ever since.","title":"References"},{"location":"terminal/pre-commits/","text":"Using pre-commit git hooks to automate code checks Git hooks are an awesome way to automate checks on your codebase locally before committing them to your code repository. That said, setting them up involves digging into the .git folder of your repository, and can feel intimidating to set up and replicate across multiple local clones of repositories. Thankfully, there is an easier way about. The developers of the pre-commit framework have given us a wonderful tool to standardize and automate the replication of pre-commit git hooks. What git hooks are Git hooks are basically commands that are run just before or after git commands are executed. In this essay's context, I basically consider it a great way to run automated checks on our code before we commit them. Getting started with pre-commit First off, you should follow the pre-commit instructions for getting setup. These instructions are availble on the pre-commit website. For those of you who know what you are doing and just want something to copy/paste: conda install -c conda-forge pre-commit pre-commit sample-config > .pre-commit-config.yaml pre-commit install pre-commit run --all-files Configuring your pre-commit While the default set is nice, you might want to install other hooks. For example, a Python project might want to default to using black as the code formatter. To enable automatic black formatting and checking before committing code, we need to add black to the configuration file that was produced ( .pre-commit-config.yaml ). - repo : https://github.com/psf/black rev : 19.3b0 hooks : - id : black A classic mistake that I made was to add black directly underneath the default: # THIS IS WRONG!!! - repo : https://github.com/pre-commit/pre-commit-hooks rev : v2.3.0 hooks : - id : check-yaml - id : end-of-file-fixer - id : trailing-whitespace - id : black # THIS IS WRONG!!! You will get an error if you do this. Be forewarned! Updating your pre-commit after updating .pre-commit-config.yaml If you forgot to add a hook but have just edited the YAML file to do so, you will need to run the command to install the hooks. pre-commit install-hooks # Optional pre-commit run --all-files Now, the new hooks will be installed. What happens when you use pre-commit As soon as you write your commit your source files, just before the commit happens, your installed pre-commit hooks execute. If the hooks modify any files, then the commit is halted, and the files that were modified will show up as being \"modified\" or \"untracked\" in your git status. At this point, add the files that were modified by your pre-commit hooks, commit those files, and re-enter your commit message. In this way, you will prevent yourself from committing code that does not pass your code checks. Good pre-commit hooks for Python projects My opinionated list of nice hooks to have can be found below. black pydocstyle isort Benefits of setting up pre-commit (and hooks) By setting up a standard configuration that gets checked into source control, we are setting our team up for success working together. Opinionated checks are now delegated to automated machinery rather than requiring human intervention, hence freeing us up to discuss higher order issues rather than nitpicking on code style. Moreover, by using the pre-commit framework, we take a lot of tedium out in setting up the pre-commit git hooks correctly. I've tried to do that before, and found writing the bash script to be a fragile task to execute. It's fragile because I'm not very proficient in Bash, and I have no other way of testing the git pre-commit hooks apart from actually making a commit. Yet, it seems like we should be able to modularize our hooks, such that they are distributed, installed, and executed in a standard fashion. This is what the pre-commit framework gives us.","title":"Using `pre-commit` git hooks to automate code checks"},{"location":"terminal/pre-commits/#using-pre-commit-git-hooks-to-automate-code-checks","text":"Git hooks are an awesome way to automate checks on your codebase locally before committing them to your code repository. That said, setting them up involves digging into the .git folder of your repository, and can feel intimidating to set up and replicate across multiple local clones of repositories. Thankfully, there is an easier way about. The developers of the pre-commit framework have given us a wonderful tool to standardize and automate the replication of pre-commit git hooks.","title":"Using pre-commit git hooks to automate code checks"},{"location":"terminal/pre-commits/#what-git-hooks-are","text":"Git hooks are basically commands that are run just before or after git commands are executed. In this essay's context, I basically consider it a great way to run automated checks on our code before we commit them.","title":"What git hooks are"},{"location":"terminal/pre-commits/#getting-started-with-pre-commit","text":"First off, you should follow the pre-commit instructions for getting setup. These instructions are availble on the pre-commit website. For those of you who know what you are doing and just want something to copy/paste: conda install -c conda-forge pre-commit pre-commit sample-config > .pre-commit-config.yaml pre-commit install pre-commit run --all-files","title":"Getting started with pre-commit"},{"location":"terminal/pre-commits/#configuring-your-pre-commit","text":"While the default set is nice, you might want to install other hooks. For example, a Python project might want to default to using black as the code formatter. To enable automatic black formatting and checking before committing code, we need to add black to the configuration file that was produced ( .pre-commit-config.yaml ). - repo : https://github.com/psf/black rev : 19.3b0 hooks : - id : black A classic mistake that I made was to add black directly underneath the default: # THIS IS WRONG!!! - repo : https://github.com/pre-commit/pre-commit-hooks rev : v2.3.0 hooks : - id : check-yaml - id : end-of-file-fixer - id : trailing-whitespace - id : black # THIS IS WRONG!!! You will get an error if you do this. Be forewarned!","title":"Configuring your pre-commit"},{"location":"terminal/pre-commits/#updating-your-pre-commit-after-updating-pre-commit-configyaml","text":"If you forgot to add a hook but have just edited the YAML file to do so, you will need to run the command to install the hooks. pre-commit install-hooks # Optional pre-commit run --all-files Now, the new hooks will be installed.","title":"Updating your pre-commit after updating .pre-commit-config.yaml"},{"location":"terminal/pre-commits/#what-happens-when-you-use-pre-commit","text":"As soon as you write your commit your source files, just before the commit happens, your installed pre-commit hooks execute. If the hooks modify any files, then the commit is halted, and the files that were modified will show up as being \"modified\" or \"untracked\" in your git status. At this point, add the files that were modified by your pre-commit hooks, commit those files, and re-enter your commit message. In this way, you will prevent yourself from committing code that does not pass your code checks.","title":"What happens when you use pre-commit"},{"location":"terminal/pre-commits/#good-pre-commit-hooks-for-python-projects","text":"My opinionated list of nice hooks to have can be found below. black pydocstyle isort","title":"Good pre-commit hooks for Python projects"},{"location":"terminal/pre-commits/#benefits-of-setting-up-pre-commit-and-hooks","text":"By setting up a standard configuration that gets checked into source control, we are setting our team up for success working together. Opinionated checks are now delegated to automated machinery rather than requiring human intervention, hence freeing us up to discuss higher order issues rather than nitpicking on code style. Moreover, by using the pre-commit framework, we take a lot of tedium out in setting up the pre-commit git hooks correctly. I've tried to do that before, and found writing the bash script to be a fragile task to execute. It's fragile because I'm not very proficient in Bash, and I have no other way of testing the git pre-commit hooks apart from actually making a commit. Yet, it seems like we should be able to modularize our hooks, such that they are distributed, installed, and executed in a standard fashion. This is what the pre-commit framework gives us.","title":"Benefits of setting up pre-commit (and hooks)"},{"location":"workflow/code-review/","text":"Practicing Code Review The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well. What code review is Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they understand the code and are able to follow the logic, find potential flaws in the newly contributed code, identify poorly documented code and confusing use of variable names, raise constructive questions and provide constructive feedback on the codebase. If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead. What code review isn't Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa. Why data scientists should do code review Reason 1: Sharing Knowledge The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.) In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model. Reason 2: Catching Mistakes The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified. Reason 3: Social Networking If your team is remote, then code review can be an incredibly powerful way of interacting with one another in a professional and constructive fashion. Because of code review, even in the absence of in-person chats, we still know someone else is looking at the product of our work. The constructive feedback and the mark of approval at the end of the code review session are little plus points that add up to a great working relationship in the long-run, and reduce the sense of loneliness in working remotely. What code review can be Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion. How code review happens Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context. As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction. Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues. Parting words If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try.","title":"Practicing Code Review"},{"location":"workflow/code-review/#practicing-code-review","text":"The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well.","title":"Practicing Code Review"},{"location":"workflow/code-review/#what-code-review-is","text":"Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they understand the code and are able to follow the logic, find potential flaws in the newly contributed code, identify poorly documented code and confusing use of variable names, raise constructive questions and provide constructive feedback on the codebase. If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead.","title":"What code review is"},{"location":"workflow/code-review/#what-code-review-isnt","text":"Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa.","title":"What code review isn't"},{"location":"workflow/code-review/#why-data-scientists-should-do-code-review","text":"","title":"Why data scientists should do code review"},{"location":"workflow/code-review/#reason-1-sharing-knowledge","text":"The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.) In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model.","title":"Reason 1: Sharing Knowledge"},{"location":"workflow/code-review/#reason-2-catching-mistakes","text":"The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified.","title":"Reason 2: Catching Mistakes"},{"location":"workflow/code-review/#reason-3-social-networking","text":"If your team is remote, then code review can be an incredibly powerful way of interacting with one another in a professional and constructive fashion. Because of code review, even in the absence of in-person chats, we still know someone else is looking at the product of our work. The constructive feedback and the mark of approval at the end of the code review session are little plus points that add up to a great working relationship in the long-run, and reduce the sense of loneliness in working remotely.","title":"Reason 3: Social Networking"},{"location":"workflow/code-review/#what-code-review-can-be","text":"Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion.","title":"What code review can be"},{"location":"workflow/code-review/#how-code-review-happens","text":"Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context. As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction. Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues.","title":"How code review happens"},{"location":"workflow/code-review/#parting-words","text":"If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try.","title":"Parting words"},{"location":"workflow/effective-commit-messages/","text":"Effective Git Commits in Data Science Continuing on the theme of the use of Git in data science, I thought I would write about how to use git commits effectively in our day-to-day data science work. How git commits are intended to be used Git commits are intended to be used as a running log of what gets checked into a code repository. In software engineering, each commit is intended to be a \u201clogical unit of work\u201d. One intent behind defining a commit as a \u201clogical unit of work\u201d is that in case that logical unit of work turned out to be faulty, we can revert that unit of work and only that unit of work without touching other units of work. Git commits can also help us track who made contributions to a repository, as each commit also contains information about the committer (e.g. name and email address). We can view the commit history at the terminal by typing the following incantation: git log --decorate --graph That will give us an interface to the commit log. It will show a running log of the commits to the project, as well as every commit message that was put in. Writing commit messages as if we're going to read them at a later date in reverse sequential order can help us write better commit messages. git commits in analysis-heavy projects In the software world, git commits are a logical way to work. By comparison, in data analysis-heavy work, it is seemingly more difficult to define a \u201clogical unit of work\u201d thank we might in software engineering. After all, what exactly constitutes a \u201clogical unit\u201d of work in data analytics? Is it the answering of a question? That might yield commits/changes that are very large. Is it a software change? That might yield commits/changes that are too small. Admittedly, there is a bit of an art to getting this right. Here, I think treating git commits more as a \"log of work done\" and less of \"report of work done\" might be helpful in adapting git as a lab notebook-style log book. Effective git commits But before we describe how, a few preliminaries are in order. L et\u2019s take a look at what effective and informative commit messages accomplish: Firstly , if we are committing something that is work-in-progress (and yes, this should be permitted, because end-of-day always rolls by), a commit message can mark the fact that there is still work to be done, and describe enough prose to resume context the next day. Secondly , when used in tandem with a timeline, an informative commit message lets us quickly isolate when work was done, thus allowing us to retrace the progression of the project. Finally , good commit messages allow others we collaborate with to get a handle on the work that was already done. Well-written git commit messages can help colleagues that review our work get quickly up-to-speed on what was done, and what to review. In other words, effective commit messages act like documentation for our future selves and for others. Once again, the \u201csocial coding\u201d paradigm comes back. Social coding? Social coding: where we aren\u2019t programming something alone, but rather writing code in collaboration with others\u2019 input. OSS development is a wonderful example of this. git commit messages: examples in data science contexts Let\u2019s see a few examples in action. The Trivial Change Message If we applied trivial changes, such as code formatting, rather than writing a message that read: Don't do this black Perhaps a a more informative message might be: Do this Applied code formatting (make format). We don\u2019t need an extended message (unlike those we might see later), because it is a trivial change. Now, I have been guilty of just writing black as the commit message, but usually that is in the context where I am working on my own project alone. Keeping in mind that commit messages are intended to be read by others, the more informative version is clearer to read and only takes practice to become second nature. The Work-In-Progress (WIP) Message Sometimes, the end of the day rolls by just like that, or we realize we have a mid-afternoon meeting to attend (these are, the wurst sausages!). In those scenarios, putting in a WIP commit may be helpful. So instead of writing a commit message that reads: Don't do this WIP loaded data We instead can write a commit message that reads: Do this WIP finished code that loads data into memory We still need to do the following: - Check statistical covariation between columns and remove correlated features. - Identify the best predictors. Now, when we look at the git log , we will see something that looks like this right at the top of our development branch: * commit abe3d2e8ed55711a57835d96e67207aa2f07f383 ( HEAD -> feature - branch ) | Author : Me < abc @ xyz . com > | Date : Fri Nov 15 14 : 01 : 13 2019 - 0500 | | WIP finished code that loads data into memory | | We still need to do the following : | | - Check statistical covariation between columns and remove correlated features . | - Identify the best predictors . | * commit ... In this way, the git commit log gives us a way to use it as a \u201clab notebook\u201d-style running log of what\u2019s we have done. The Report on Progress Pedantically, this is distinguished from the WIP message described above by being a \u201cfinal\u201d (but not necessarily binding) message in the work log. An uninformative commit message for this would look like: Don't do this Finally done with model building By contrast, an informative one might look something like this: Do this ```text Model building (Issue #34) ready for review Finished: Pipeline taking data from input (strings) to activity prediction. Custom code for data pipeline has been stored in custom package. Tests and docs written. Notebooks documenting work are also written. Static HTML version for archival also generated. Not done: Hyperparameter selection. This is the logical next step, and as agreed at last meeting, of highest priority. ```text Admittedly, it can be tough to know when to write this one, and I think it\u2019s because it feels like we might want to be sure that this is absolutely the place that we actually want to write such a message. To this, I would suggest simply commit (pun intended) to writing it when appropriate, and worry about minor things in later commits. Squashed commits If we squash commits in our git workflow (e.g. when merging branches), then writing such detailed commit messages might seem unnecessary. To which my response is, yes indeed! In the case of using squashed commits really only the final commit message ends up being stored in the running log of what gets done. Hence, it makes perfect sense to focus writing good commit messages only at the merge stage, rather than at every single commit. Intentional adoption of better commit messages As I have observed with my own and colleagues\u2019 workflows, we do not regularly write informative commit messages because we don\u2019t read the git log. Then again, we don\u2019t read the git log because it doesn\u2019t contain a lot of information. Hold on, that sounds kind of circular, doesn\u2019t it? I think the chicken-and-egg cycle at some point has to be broken. By starting at some point, we break a vicious cycle of uninformative logging, and allow us to break into a virtuous cycle of good record-keeping. And that really is what this essay is trying to encourage: better record-keeping! Further Reading How to Write a Git Commit Message by Chris Beams . A note to Chris Thank you for writing a wonderful article. I'll be praying for a speedy recovery, Chris.","title":"Effective Git Commits in Data Science"},{"location":"workflow/effective-commit-messages/#effective-git-commits-in-data-science","text":"Continuing on the theme of the use of Git in data science, I thought I would write about how to use git commits effectively in our day-to-day data science work.","title":"Effective Git Commits in Data Science"},{"location":"workflow/effective-commit-messages/#how-git-commits-are-intended-to-be-used","text":"Git commits are intended to be used as a running log of what gets checked into a code repository. In software engineering, each commit is intended to be a \u201clogical unit of work\u201d. One intent behind defining a commit as a \u201clogical unit of work\u201d is that in case that logical unit of work turned out to be faulty, we can revert that unit of work and only that unit of work without touching other units of work. Git commits can also help us track who made contributions to a repository, as each commit also contains information about the committer (e.g. name and email address). We can view the commit history at the terminal by typing the following incantation: git log --decorate --graph That will give us an interface to the commit log. It will show a running log of the commits to the project, as well as every commit message that was put in. Writing commit messages as if we're going to read them at a later date in reverse sequential order can help us write better commit messages.","title":"How git commits are intended to be used"},{"location":"workflow/effective-commit-messages/#git-commits-in-analysis-heavy-projects","text":"In the software world, git commits are a logical way to work. By comparison, in data analysis-heavy work, it is seemingly more difficult to define a \u201clogical unit of work\u201d thank we might in software engineering. After all, what exactly constitutes a \u201clogical unit\u201d of work in data analytics? Is it the answering of a question? That might yield commits/changes that are very large. Is it a software change? That might yield commits/changes that are too small. Admittedly, there is a bit of an art to getting this right. Here, I think treating git commits more as a \"log of work done\" and less of \"report of work done\" might be helpful in adapting git as a lab notebook-style log book.","title":"git commits in analysis-heavy projects"},{"location":"workflow/effective-commit-messages/#effective-git-commits","text":"But before we describe how, a few preliminaries are in order. L et\u2019s take a look at what effective and informative commit messages accomplish: Firstly , if we are committing something that is work-in-progress (and yes, this should be permitted, because end-of-day always rolls by), a commit message can mark the fact that there is still work to be done, and describe enough prose to resume context the next day. Secondly , when used in tandem with a timeline, an informative commit message lets us quickly isolate when work was done, thus allowing us to retrace the progression of the project. Finally , good commit messages allow others we collaborate with to get a handle on the work that was already done. Well-written git commit messages can help colleagues that review our work get quickly up-to-speed on what was done, and what to review. In other words, effective commit messages act like documentation for our future selves and for others. Once again, the \u201csocial coding\u201d paradigm comes back. Social coding? Social coding: where we aren\u2019t programming something alone, but rather writing code in collaboration with others\u2019 input. OSS development is a wonderful example of this.","title":"Effective git commits"},{"location":"workflow/effective-commit-messages/#git-commit-messages-examples-in-data-science-contexts","text":"Let\u2019s see a few examples in action.","title":"git commit messages: examples in data science contexts"},{"location":"workflow/effective-commit-messages/#the-trivial-change-message","text":"If we applied trivial changes, such as code formatting, rather than writing a message that read: Don't do this black Perhaps a a more informative message might be: Do this Applied code formatting (make format). We don\u2019t need an extended message (unlike those we might see later), because it is a trivial change. Now, I have been guilty of just writing black as the commit message, but usually that is in the context where I am working on my own project alone. Keeping in mind that commit messages are intended to be read by others, the more informative version is clearer to read and only takes practice to become second nature.","title":"The Trivial Change Message"},{"location":"workflow/effective-commit-messages/#the-work-in-progress-wip-message","text":"Sometimes, the end of the day rolls by just like that, or we realize we have a mid-afternoon meeting to attend (these are, the wurst sausages!). In those scenarios, putting in a WIP commit may be helpful. So instead of writing a commit message that reads: Don't do this WIP loaded data We instead can write a commit message that reads: Do this WIP finished code that loads data into memory We still need to do the following: - Check statistical covariation between columns and remove correlated features. - Identify the best predictors. Now, when we look at the git log , we will see something that looks like this right at the top of our development branch: * commit abe3d2e8ed55711a57835d96e67207aa2f07f383 ( HEAD -> feature - branch ) | Author : Me < abc @ xyz . com > | Date : Fri Nov 15 14 : 01 : 13 2019 - 0500 | | WIP finished code that loads data into memory | | We still need to do the following : | | - Check statistical covariation between columns and remove correlated features . | - Identify the best predictors . | * commit ... In this way, the git commit log gives us a way to use it as a \u201clab notebook\u201d-style running log of what\u2019s we have done.","title":"The Work-In-Progress (WIP) Message"},{"location":"workflow/effective-commit-messages/#the-report-on-progress","text":"Pedantically, this is distinguished from the WIP message described above by being a \u201cfinal\u201d (but not necessarily binding) message in the work log. An uninformative commit message for this would look like: Don't do this Finally done with model building By contrast, an informative one might look something like this: Do this ```text Model building (Issue #34) ready for review Finished: Pipeline taking data from input (strings) to activity prediction. Custom code for data pipeline has been stored in custom package. Tests and docs written. Notebooks documenting work are also written. Static HTML version for archival also generated. Not done: Hyperparameter selection. This is the logical next step, and as agreed at last meeting, of highest priority. ```text Admittedly, it can be tough to know when to write this one, and I think it\u2019s because it feels like we might want to be sure that this is absolutely the place that we actually want to write such a message. To this, I would suggest simply commit (pun intended) to writing it when appropriate, and worry about minor things in later commits.","title":"The Report on Progress"},{"location":"workflow/effective-commit-messages/#squashed-commits","text":"If we squash commits in our git workflow (e.g. when merging branches), then writing such detailed commit messages might seem unnecessary. To which my response is, yes indeed! In the case of using squashed commits really only the final commit message ends up being stored in the running log of what gets done. Hence, it makes perfect sense to focus writing good commit messages only at the merge stage, rather than at every single commit.","title":"Squashed commits"},{"location":"workflow/effective-commit-messages/#intentional-adoption-of-better-commit-messages","text":"As I have observed with my own and colleagues\u2019 workflows, we do not regularly write informative commit messages because we don\u2019t read the git log. Then again, we don\u2019t read the git log because it doesn\u2019t contain a lot of information. Hold on, that sounds kind of circular, doesn\u2019t it? I think the chicken-and-egg cycle at some point has to be broken. By starting at some point, we break a vicious cycle of uninformative logging, and allow us to break into a virtuous cycle of good record-keeping. And that really is what this essay is trying to encourage: better record-keeping!","title":"Intentional adoption of better commit messages"},{"location":"workflow/effective-commit-messages/#further-reading","text":"How to Write a Git Commit Message by Chris Beams . A note to Chris Thank you for writing a wonderful article. I'll be praying for a speedy recovery, Chris.","title":"Further Reading"},{"location":"workflow/gitflow/","text":"Principled Git-based Workflow in Collaborative Data Science Projects GitFlow is an incredible branching model for working with code. In this essay, I would like to introduce it to you, the data scientist, and show how it might be useful in your context, especially for working with multiple colleagues on the same project. What GitFlow is GitFlow is a way of working with multiple collaborators on a git repository. It originated in the software development world, and gives software developers a way of keeping new development work isolated from reviewed, documented, and stable code. At its core, we have a \"source of truth\" branch called master , from which we make branches on which development work happens. Development work basically means new code, added documentation, more tests, etc. When the new code, documentation, tests, and more are reviewed, a pull request is made to merge the new code back into the master branch. Usually, the act of making a branch is paired with raising an issue on an issue tracker, in which the problem and proposed solution are written down. (In other words, the deliverables are explicitly sketched out.) Merging into master is paired with a code review session, in which another colleague (or the tech lead) reviews the code to be merged, and approves (or denies) code merger based on whether the issue raised in the issue tracker has been resolved. From my time experimenting with GitFlow at work, I think that when paired with other principled workflows that doen't directly interact with Git, can I think be of great utility to data scientists. It does, however, involve a bit of change in the common mode of working that data scientists use. Is GitFlow still confusing for you? If so, please check out this article on GitFlow. It includes the appropriate graphics that will make it much clearer. I felt that a detailed explanation here would be rather out of scope. That said, nothing beats trying it out to get a feel for it, so if you're willing to pick it up, I would encourage you to find a software developer in your organization who has experience with GitFlow and ask them to guide you on it. GitFlow in a data science project Here is how I think GitFlow can be successfully deployed in a data science project. Everything starts with the unit of analysis that we are trying to perform. We start by defining the question that we are trying to answer. We then proceed forward by sketching out an analysis plan (let's call this an analysis sketch ), which outlines the data sources that we need, the strategy for analyzing the data (roughly including: models we think might be relevant to the scale of the problem, the plots we think might be relevant to make, and where we think, future directions might lie). None of this is binding, which makes the analysis sketch less like a formal pre-registered analysis plan, and more like a tool to be more thoughtful of what we want to do when analyzing our data. After all, one of the myths of data science is that we can \"stir the pile until the data start looking right\" . About stirring the pot... If you didn't click the URL to go to XKCD, here's the cartoon embedded below: Once we are done with defining the analysis sketch in an issue, we follow the rest of GitFlow-based workflow: We create a branch off from master , execute on our work, and submit a pull request with everything that we have done. We then invite a colleague to review our work, in which the colleague is explicitly checking that we have delivered on our analysis sketch, or if we have changed course, to discuss the analysis with us in a formal setting. Ideally this is done in-person, but by submitting a formal pull request, our colleague can pull down our code and check that things have been done correctly on their computer. Code review If you want to know more about code review, please check out another essay in this collection. If your team has access to a Binder -like service, then review can be done in an even simpler fashion: simply create a Binder session for the colleague's fork, and explore the analyses there in a temporary session. Once the formal review has finished and both colleagues are on the same page with the analysis, the analysis is merged back into the master branch, and considered done. Both parties can now move onto the next analysis. Mindset changes needed to make GitFlow work In this section, I am going to describe some common mindsets that prevent successful adoption of GitFlow that data scientists might employ, and ways to adapt those mindsets to work with GitFlow. Jumping straight into exploratory data analysis (EDA) This is a common one that even I have done before. The refrain in our mind is, \"Just give me the CSV file! I will figure something out.\" Famous last words, once we come to terms with the horror that we experience in looking through the data. It seems, though, that we shouldn't be able to sketch an analysis plan for EDA, right? I think that mode of thinking might be a tad pessimistic. What we are trying to accomplish with exploratory data analysis is to establish our own working knowledge on: The bounds of the data, The types of the data (ordinal, categorical, numeric), The possible definitions of a single sample in the dataset, Covariation between columns of data, Whether or not the data can answer our questions, and Further questions that come up while looking at the data. Hence, a good analysis sketch to raise for exploratory data analysis would be to write a Jupyter notebook that simply documents all of the above, and then have a colleague review it. Endless modelling experiments This is another one of those trops that I fall into often, so I am sympathetic towards others who might do the same. Scientists (of any type, not just data sciensists) usually come with an obsessive streak, and the way it manifests in data science is usually the quest for the best-performing model. However, in most data science settings, the goal we are trying to accomplish requires first proving out the value of our work using some form of prototype, so we cannot afford to chase performance rabbits down their hole. One way to get around this is to think about the problem in two phases. The first phase is model prototyping . As such, in the analysis sketch, we define a deliverable that is \"a machine learning model that predicts Y from X\", leaving out the performance metric for now . In other words, we are establishing a baseline model, and building out the analysis framework for evaluating how good the model is in the larger applied context. We do this in a quick and dirty fashion, and invite a colleague to review our work to ensure that we have not made any elementary statistical errors, and that the framework is correct with respect to the applied problem that we are tackling. (See note below for more detail.) Note: statistical errors For example, we need to get splitting done correctly in a time series setting, which does not have i.i.d. samples, compared to most other ML problems. And in a cheminformatics setting, random splits tend to over-estimate model performance when compared to a real-world setting where new molecules are often out-of-distribution. If we focused on getting a good model right from the get-go, we may end up missing out on elementary details such as these. Once we are done with this, we embark on the second phase, which is model improvement . Here, we define another analysis sketch where we outline the models that we intend to try, and for which the deliverable is now a Jupyter notebook documenting the modelling experiments we tried. As usual, once we are done, we invite a colleague to review the work to make sure that we have conducted it correctly. A key here is to define the task in as neutral and relevant terms as possible. For example, nobody can guarantee an improved model. However, we can promise a comprehensive, if not exhaustive, search through model and parameter space. We can also guarantee delivering recommendations for improvement regardless of what model performance looks like. Note: Neutral forms of goals As expressed on Twitter before, \"the most scary scientist is one with a hypothesis to prove\". A data scientist who declares that a high-performing model will be the goal is probably being delusional. I wish I knew where exactly I saw the quote, and hence will not take credit for that. Endless ideation prototyping Another trap I have fallen into involves endless ideation prototyping, which is very similar to the \"endless modelling experiments\" problem described above. My proposal here, then, is two-fold. Firstly, rather than running down rabbit holes endlessly, we trust our instincts in evaluating the maturity of an idea. Secondly, we ought also to define \"kill/stop criteria\" ahead-of-time, and move as quickly as possible to kill the idea while also documenting it in a Jupyter notebook. If made part of an analysis sketch that is raised on the issue tracker, then we can be kept accountable by our colleagues. Benefits of adopting GitFlow and associated practices At its core, adopting a workflow as described above is really about intentionally slowing down our work a little bit so that we are more thoughtful about the work we want to finish. In work with my colleagues, I have found this to be incredibly useful. GitFlow and its associated practices bring a suite of benefits to our projects, and I think it is easy to see how. By spending a bit more time on thought and on execution, we cut down on wasted hours exploring unproductive analysis avenues. By pre-defining deliverables expressed in a neutral form, we reduce stress and pressure on data scientists, We also prevent endless rabbit-hole hacking to achieve those non-neutrally-expressed goals. We also receive a less biased analysis, which I believe can only help with making better decisions. Finally, by inviting colleagues to review our work, we also prevent the silo-ing of knowledge on one person, and instead distribute expertise and knowledge. How to gradually adopt GitFlow in your data science teams I know that not every single data science team will have adopted GitFlow from the get-go, and so there will have to be some form of ramp-up to get it going productively. Because this is a collaborative workflow, and because adoption is usually done only in the presence of incentives, I think that in order for GitFlow and associated practices to be adopted, one or more champions for using GitFlow needs to be empowered with the authority to use this workflow on any project they embark on. They also have to be sufficiently unpressured to deliver, so that time and performance pressures do not compromise on adoption. Finally, they have to be able to teach git newcomers and debug problems that show up in git branching, and be able to handle the git workflow for colleagues who might not have the time to pick it up. Tooling also has to be present. A modern version control system and associated hosting software, such as BitBucket, GitHub and GitLab, are necessary. Issue trackers also need to be present for each repository (or project, more generally). At my workplace, I have been fortunate to initiate two projects on which we practice GitFlow, bringing along an intern and a colleague one rank above me who were willing to try this out. This has led to much better sharing of the coding and knowledge load, and has also allowed us to cover for one another much more effectively. While above I may have sounded as if there is resistance to adoption, in practice I know that most data scientists instinctively know that proper workflows are going to be highly beneficial, but lack the time/space and incentives to introduce them in, yet would jump at the chance to do so if properly incentivized and given the time and space to do so. Concluding words I hope that I have convinced you that learning GitFlow, and its associated practices, can be incredibly useful for the long-term health and productivity of your data science team(s).","title":"Principled Git-based Workflow in Collaborative Data Science Projects"},{"location":"workflow/gitflow/#principled-git-based-workflow-in-collaborative-data-science-projects","text":"GitFlow is an incredible branching model for working with code. In this essay, I would like to introduce it to you, the data scientist, and show how it might be useful in your context, especially for working with multiple colleagues on the same project.","title":"Principled Git-based Workflow in Collaborative Data Science Projects"},{"location":"workflow/gitflow/#what-gitflow-is","text":"GitFlow is a way of working with multiple collaborators on a git repository. It originated in the software development world, and gives software developers a way of keeping new development work isolated from reviewed, documented, and stable code. At its core, we have a \"source of truth\" branch called master , from which we make branches on which development work happens. Development work basically means new code, added documentation, more tests, etc. When the new code, documentation, tests, and more are reviewed, a pull request is made to merge the new code back into the master branch. Usually, the act of making a branch is paired with raising an issue on an issue tracker, in which the problem and proposed solution are written down. (In other words, the deliverables are explicitly sketched out.) Merging into master is paired with a code review session, in which another colleague (or the tech lead) reviews the code to be merged, and approves (or denies) code merger based on whether the issue raised in the issue tracker has been resolved. From my time experimenting with GitFlow at work, I think that when paired with other principled workflows that doen't directly interact with Git, can I think be of great utility to data scientists. It does, however, involve a bit of change in the common mode of working that data scientists use. Is GitFlow still confusing for you? If so, please check out this article on GitFlow. It includes the appropriate graphics that will make it much clearer. I felt that a detailed explanation here would be rather out of scope. That said, nothing beats trying it out to get a feel for it, so if you're willing to pick it up, I would encourage you to find a software developer in your organization who has experience with GitFlow and ask them to guide you on it.","title":"What GitFlow is"},{"location":"workflow/gitflow/#gitflow-in-a-data-science-project","text":"Here is how I think GitFlow can be successfully deployed in a data science project. Everything starts with the unit of analysis that we are trying to perform. We start by defining the question that we are trying to answer. We then proceed forward by sketching out an analysis plan (let's call this an analysis sketch ), which outlines the data sources that we need, the strategy for analyzing the data (roughly including: models we think might be relevant to the scale of the problem, the plots we think might be relevant to make, and where we think, future directions might lie). None of this is binding, which makes the analysis sketch less like a formal pre-registered analysis plan, and more like a tool to be more thoughtful of what we want to do when analyzing our data. After all, one of the myths of data science is that we can \"stir the pile until the data start looking right\" . About stirring the pot... If you didn't click the URL to go to XKCD, here's the cartoon embedded below: Once we are done with defining the analysis sketch in an issue, we follow the rest of GitFlow-based workflow: We create a branch off from master , execute on our work, and submit a pull request with everything that we have done. We then invite a colleague to review our work, in which the colleague is explicitly checking that we have delivered on our analysis sketch, or if we have changed course, to discuss the analysis with us in a formal setting. Ideally this is done in-person, but by submitting a formal pull request, our colleague can pull down our code and check that things have been done correctly on their computer. Code review If you want to know more about code review, please check out another essay in this collection. If your team has access to a Binder -like service, then review can be done in an even simpler fashion: simply create a Binder session for the colleague's fork, and explore the analyses there in a temporary session. Once the formal review has finished and both colleagues are on the same page with the analysis, the analysis is merged back into the master branch, and considered done. Both parties can now move onto the next analysis.","title":"GitFlow in a data science project"},{"location":"workflow/gitflow/#mindset-changes-needed-to-make-gitflow-work","text":"In this section, I am going to describe some common mindsets that prevent successful adoption of GitFlow that data scientists might employ, and ways to adapt those mindsets to work with GitFlow.","title":"Mindset changes needed to make GitFlow work"},{"location":"workflow/gitflow/#jumping-straight-into-exploratory-data-analysis-eda","text":"This is a common one that even I have done before. The refrain in our mind is, \"Just give me the CSV file! I will figure something out.\" Famous last words, once we come to terms with the horror that we experience in looking through the data. It seems, though, that we shouldn't be able to sketch an analysis plan for EDA, right? I think that mode of thinking might be a tad pessimistic. What we are trying to accomplish with exploratory data analysis is to establish our own working knowledge on: The bounds of the data, The types of the data (ordinal, categorical, numeric), The possible definitions of a single sample in the dataset, Covariation between columns of data, Whether or not the data can answer our questions, and Further questions that come up while looking at the data. Hence, a good analysis sketch to raise for exploratory data analysis would be to write a Jupyter notebook that simply documents all of the above, and then have a colleague review it.","title":"Jumping straight into exploratory data analysis (EDA)"},{"location":"workflow/gitflow/#endless-modelling-experiments","text":"This is another one of those trops that I fall into often, so I am sympathetic towards others who might do the same. Scientists (of any type, not just data sciensists) usually come with an obsessive streak, and the way it manifests in data science is usually the quest for the best-performing model. However, in most data science settings, the goal we are trying to accomplish requires first proving out the value of our work using some form of prototype, so we cannot afford to chase performance rabbits down their hole. One way to get around this is to think about the problem in two phases. The first phase is model prototyping . As such, in the analysis sketch, we define a deliverable that is \"a machine learning model that predicts Y from X\", leaving out the performance metric for now . In other words, we are establishing a baseline model, and building out the analysis framework for evaluating how good the model is in the larger applied context. We do this in a quick and dirty fashion, and invite a colleague to review our work to ensure that we have not made any elementary statistical errors, and that the framework is correct with respect to the applied problem that we are tackling. (See note below for more detail.) Note: statistical errors For example, we need to get splitting done correctly in a time series setting, which does not have i.i.d. samples, compared to most other ML problems. And in a cheminformatics setting, random splits tend to over-estimate model performance when compared to a real-world setting where new molecules are often out-of-distribution. If we focused on getting a good model right from the get-go, we may end up missing out on elementary details such as these. Once we are done with this, we embark on the second phase, which is model improvement . Here, we define another analysis sketch where we outline the models that we intend to try, and for which the deliverable is now a Jupyter notebook documenting the modelling experiments we tried. As usual, once we are done, we invite a colleague to review the work to make sure that we have conducted it correctly. A key here is to define the task in as neutral and relevant terms as possible. For example, nobody can guarantee an improved model. However, we can promise a comprehensive, if not exhaustive, search through model and parameter space. We can also guarantee delivering recommendations for improvement regardless of what model performance looks like. Note: Neutral forms of goals As expressed on Twitter before, \"the most scary scientist is one with a hypothesis to prove\". A data scientist who declares that a high-performing model will be the goal is probably being delusional. I wish I knew where exactly I saw the quote, and hence will not take credit for that.","title":"Endless modelling experiments"},{"location":"workflow/gitflow/#endless-ideation-prototyping","text":"Another trap I have fallen into involves endless ideation prototyping, which is very similar to the \"endless modelling experiments\" problem described above. My proposal here, then, is two-fold. Firstly, rather than running down rabbit holes endlessly, we trust our instincts in evaluating the maturity of an idea. Secondly, we ought also to define \"kill/stop criteria\" ahead-of-time, and move as quickly as possible to kill the idea while also documenting it in a Jupyter notebook. If made part of an analysis sketch that is raised on the issue tracker, then we can be kept accountable by our colleagues.","title":"Endless ideation prototyping"},{"location":"workflow/gitflow/#benefits-of-adopting-gitflow-and-associated-practices","text":"At its core, adopting a workflow as described above is really about intentionally slowing down our work a little bit so that we are more thoughtful about the work we want to finish. In work with my colleagues, I have found this to be incredibly useful. GitFlow and its associated practices bring a suite of benefits to our projects, and I think it is easy to see how. By spending a bit more time on thought and on execution, we cut down on wasted hours exploring unproductive analysis avenues. By pre-defining deliverables expressed in a neutral form, we reduce stress and pressure on data scientists, We also prevent endless rabbit-hole hacking to achieve those non-neutrally-expressed goals. We also receive a less biased analysis, which I believe can only help with making better decisions. Finally, by inviting colleagues to review our work, we also prevent the silo-ing of knowledge on one person, and instead distribute expertise and knowledge.","title":"Benefits of adopting GitFlow and associated practices"},{"location":"workflow/gitflow/#how-to-gradually-adopt-gitflow-in-your-data-science-teams","text":"I know that not every single data science team will have adopted GitFlow from the get-go, and so there will have to be some form of ramp-up to get it going productively. Because this is a collaborative workflow, and because adoption is usually done only in the presence of incentives, I think that in order for GitFlow and associated practices to be adopted, one or more champions for using GitFlow needs to be empowered with the authority to use this workflow on any project they embark on. They also have to be sufficiently unpressured to deliver, so that time and performance pressures do not compromise on adoption. Finally, they have to be able to teach git newcomers and debug problems that show up in git branching, and be able to handle the git workflow for colleagues who might not have the time to pick it up. Tooling also has to be present. A modern version control system and associated hosting software, such as BitBucket, GitHub and GitLab, are necessary. Issue trackers also need to be present for each repository (or project, more generally). At my workplace, I have been fortunate to initiate two projects on which we practice GitFlow, bringing along an intern and a colleague one rank above me who were willing to try this out. This has led to much better sharing of the coding and knowledge load, and has also allowed us to cover for one another much more effectively. While above I may have sounded as if there is resistance to adoption, in practice I know that most data scientists instinctively know that proper workflows are going to be highly beneficial, but lack the time/space and incentives to introduce them in, yet would jump at the chance to do so if properly incentivized and given the time and space to do so.","title":"How to gradually adopt GitFlow in your data science teams"},{"location":"workflow/gitflow/#concluding-words","text":"I hope that I have convinced you that learning GitFlow, and its associated practices, can be incredibly useful for the long-term health and productivity of your data science team(s).","title":"Concluding words"}]}